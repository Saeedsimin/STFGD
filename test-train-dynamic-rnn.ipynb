{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from utils import *\n",
    "from data import *\n",
    "import json\n",
    "import numpy as np \n",
    "import networkx as nx\n",
    "import os\n",
    "import csv \n",
    "max_nodes = 15\n",
    "name = 'YELP'\n",
    "## Loading the main graph \n",
    "# graphs = create_graphs.create(args)\n",
    "print('Loading graph dataset: '+str(name))\n",
    "G = nx.Graph()\n",
    "\n",
    "path = 'dataset/'+name+'/'\n",
    "data_ids_file = open(path+name+'_user_id_map_inverse.txt','r')\n",
    "data_ids = json.load(data_ids_file)\n",
    "data_ids_map_file = open(path + name + '_user_id_map_inverse.txt','r')\n",
    "data_ids_map = json.load(data_ids_map_file)\n",
    "data_adj = np.loadtxt(path+name+'_A.txt', delimiter=',').astype(int)\n",
    "data_node_att = np.loadtxt(path+name+'_node_attributes.txt', delimiter=',')\n",
    "data_node_label = np.loadtxt(path+name+'_node_labels.txt', delimiter=',').astype(int)\n",
    "data_graph_indicator_org = np.loadtxt(path+name+'_graph_indicator.txt', delimiter=',').astype(int)\n",
    "data_graph_labels = np.loadtxt(path+name+'_graph_labels.txt', delimiter=',').astype(int)\n",
    "data_tuple = list(map(tuple, data_adj))\n",
    "\n",
    "# create graph-\n",
    "G.add_edges_from(data_tuple)\n",
    "graph_num = data_graph_indicator_org.max()\n",
    "count_id = 0\n",
    "data_graph_indicator = []\n",
    "graph_num_list = [0 for _ in range(graph_num)]\n",
    "for i in range(data_node_label.shape[0]):\n",
    "    node_group_id = data_graph_indicator_org[i]\n",
    "    if graph_num_list[node_group_id-1] < max_nodes:\n",
    "        graph_num_list[node_group_id-1] += 1\n",
    "        G.add_node(count_id+1, feature = data_node_att[i])\n",
    "        G.add_node(count_id+1, label = data_node_label[i])\n",
    "        G.add_node(count_id+1, user_id = data_ids[str(i+1)])\n",
    "        G.add_node(count_id+1, group_id = data_graph_indicator_org[i])\n",
    "        data_graph_indicator.append(data_graph_indicator_org[i])\n",
    "        count_id += 1\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "data_graph_indicator = np.array(data_graph_indicator)\n",
    "node_list = np.arange(count_id) + 1\n",
    "graphs = []\n",
    "total_num_of_nodes = 0\n",
    "for i in range(graph_num):    \n",
    "    # find the nodes for each graph\n",
    "    nodes = node_list[data_graph_indicator==i+1]\n",
    "    if len(nodes)>max_nodes:\n",
    "        nodes = nodes[0:max_nodes]\n",
    "    total_num_of_nodes += len(nodes)\n",
    "    G_sub = G.subgraph(nodes)\n",
    "    G_attributes = nx.get_node_attributes(G_sub,'feature')\n",
    "    G_sub.graph['label'] = data_graph_labels[i]\n",
    "    graphs.append(G_sub)\n",
    "    G_ids = nx.get_node_attributes(G_sub,'user_id')\n",
    "#     print(G_ids)\n",
    "#     if G_sub.number_of_nodes() > max_nodes:\n",
    "#         max_nodes = G_sub.number_of_nodes()\n",
    "# shuffling the data \n",
    "# print(total_num_of_nodes)\n",
    "\n",
    "graphs_len = len(graphs)\n",
    "# random.seed(123)\n",
    "# graphs_list = list(enumerate(graphs))\n",
    "# shuffle(graphs_list)\n",
    "# graphs_idx, graphs = zip(*graphs_list)\n",
    "\n",
    "graphs_test = graphs[int(0.8 * graphs_len):]\n",
    "graphs_test_len = len(graphs_test)\n",
    "graphs_test_dic = {}\n",
    "nodes_test_len = 0\n",
    "test_id = 0\n",
    "for graph in graphs_test:\n",
    "    node_ids = nx.get_node_attributes(graph,'user_id')\n",
    "    for key, value in node_ids.items():\n",
    "        test_id += 1\n",
    "        graphs_test_dic[value] = test_id\n",
    "        nodes_test_len += 1\n",
    "    \n",
    "graphs_train = graphs[0:int(0.8*graphs_len)]\n",
    "# graphs_train_idx = graphs_idx[0:int(0.8*graphs_len)]\n",
    "graphs_train_len = len(graphs_train)\n",
    "graphs_train_dic = {}\n",
    "nodes_train_len = 0\n",
    "train_id = 0\n",
    "for graph in graphs_train:\n",
    "    node_ids = nx.get_node_attributes(graph,'user_id')\n",
    "    for key, value in node_ids.items():\n",
    "        train_id += 1 \n",
    "        graphs_train_dic[value] = train_id\n",
    "        nodes_train_len += 1\n",
    "# print(graphs_train_dic)\n",
    "graphs_validate = graphs[0:int(0.2*graphs_len)]\n",
    "\n",
    "num_of_intervals = 0\n",
    "dynamic_graphs_path = path + 'dynamic_graphs/'\n",
    "for _, dirnames,_ in os.walk(dynamic_graphs_path):\n",
    "    num_of_intervals += len(dirnames)\n",
    "print(num_of_intervals)\n",
    "num_of_groups = []\n",
    "\n",
    "\n",
    "# Train data extraction\n",
    "train_data = []\n",
    "for i in range(num_of_intervals-1):\n",
    "    train_adj_mat = np.zeros((nodes_train_len,max_nodes))\n",
    "    read_path = dynamic_graphs_path + str(i+1) + '/'\n",
    "    data_ids_file = open(read_path + name +'_user_id_map.txt','r')\n",
    "    data_ids_dyn = json.load(data_ids_file)\n",
    "    data_ids_inverse_dyn = dict([(value, key) for key, value in data_ids_dyn.items()]) \n",
    "    data_adj_dyn = np.loadtxt(read_path+name+'_A.txt', delimiter=',').astype(int)\n",
    "    data_graph_indicator_dyn = np.loadtxt(read_path+name+'_graph_indicator.txt', delimiter=',').astype(int)  \n",
    "    if len(data_adj_dyn) != 0:\n",
    "        num_of_groups.append(len(np.unique(data_adj_dyn)))\n",
    "    else:\n",
    "        num_of_groups.append(0)\n",
    "    if len(data_adj_dyn) != 0:\n",
    "        data_tuple_dyn = list(map(tuple, data_adj_dyn))\n",
    "        for tuple_item in data_tuple_dyn:\n",
    "            main_idx = tuple_item[1]\n",
    "            neighbor_idx = tuple_item[0]\n",
    "            group_id = data_graph_indicator_dyn[main_idx-1]\n",
    "#             if group_id in graphs_train_idx:\n",
    "            if group_id < graphs_train_len:\n",
    "#                 print(group_id, graphs_train_idx.index(group_id))\n",
    "                user_id_main = data_ids_inverse_dyn[main_idx]\n",
    "                user_id_neighbor = data_ids_inverse_dyn[neighbor_idx]\n",
    "#                 group_idx = graphs_train_idx.index(group_id)\n",
    "                user_group = graphs_train[group_id - 1]\n",
    "                for key,value in graphs_train_dic.items():\n",
    "                    if key == user_id_main:                        \n",
    "                        main_user_idx = value\n",
    "                        break\n",
    "                        \n",
    "                user_ids = nx.get_node_attributes(user_group,'user_id')                \n",
    "                counter = 0\n",
    "                for key,value in user_ids.items():\n",
    "                    if user_ids[key] == user_id_neighbor and counter<max_nodes:                        \n",
    "                        train_adj_mat[main_user_idx-1][counter] = 1\n",
    "                        break\n",
    "                    counter += 1\n",
    "        train_data.append(train_adj_mat)\n",
    "\n",
    "#     for m in range(len(train_adj_mat)):\n",
    "#         for n in range(len(train_adj_mat[0])):\n",
    "#             if train_adj_mat[m][n] == 1:\n",
    "#                 print(i,' ',m,' ',n)\n",
    "                \n",
    "# Test data extraction\n",
    "test_data = []\n",
    "for i in range(num_of_intervals-1):\n",
    "    test_adj_mat = np.zeros((nodes_test_len,max_nodes))\n",
    "    read_path = dynamic_graphs_path + str(i+1) + '/'\n",
    "    data_ids_file = open(read_path + name +'_user_id_map.txt','r')\n",
    "    data_ids_dyn = json.load(data_ids_file)\n",
    "    data_ids_inverse_dyn = dict([(value, key) for key, value in data_ids_dyn.items()]) \n",
    "    data_adj_dyn = np.loadtxt(read_path+name+'_A.txt', delimiter=',').astype(int)\n",
    "    data_graph_indicator_dyn = np.loadtxt(read_path+name+'_graph_indicator.txt', delimiter=',').astype(int)  \n",
    "    if len(data_adj_dyn) != 0:\n",
    "        num_of_groups.append(len(np.unique(data_adj_dyn)))\n",
    "    else:\n",
    "        num_of_groups.append(0)\n",
    "    if len(data_adj_dyn) != 0:\n",
    "        data_tuple_dyn = list(map(tuple, data_adj_dyn))\n",
    "        for tuple_item in data_tuple_dyn:\n",
    "            main_idx = tuple_item[1]\n",
    "            neighbor_idx = tuple_item[0]\n",
    "            group_id = data_graph_indicator_dyn[main_idx-1]\n",
    "#             if group_id in graphs_train_idx:\n",
    "            if group_id >= graphs_train_len:\n",
    "#                 print(group_id, graphs_train_idx.index(group_id))\n",
    "                user_id_main = data_ids_inverse_dyn[main_idx]\n",
    "                user_id_neighbor = data_ids_inverse_dyn[neighbor_idx]\n",
    "#                 group_idx = graphs_train_idx.index(group_id)\n",
    "                user_group = graphs[group_id - 1]\n",
    "                for key,value in graphs_test_dic.items():\n",
    "                    if key == user_id_main:                        \n",
    "                        main_user_idx = value\n",
    "                        break\n",
    "                        \n",
    "                user_ids = nx.get_node_attributes(user_group,'user_id')                \n",
    "                counter = 0\n",
    "                for key,value in user_ids.items():\n",
    "                    if user_ids[key] == user_id_neighbor and counter<max_nodes:                        \n",
    "                        test_adj_mat[main_user_idx-1][counter] = 1\n",
    "                        break\n",
    "                    counter += 1\n",
    "        test_data.append(test_adj_mat)\n",
    "\n",
    "#     for m in range(len(test_adj_mat)):\n",
    "#         for n in range(len(train_adj_mat[0])):\n",
    "#             if train_adj_mat[m][n] == 1:\n",
    "#                 print(i,' ',m,' ',n)\n",
    "\n",
    "\n",
    "    \n",
    "# with open('output.csv','w') as result_file:\n",
    "#     wr = csv.writer(result_file, dialect='excel')\n",
    "#     wr.writerow(num_of_groups)               \n",
    "                    \n",
    "                    \n",
    "            \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "total_epoch = 10\n",
    "current_epoch = 0\n",
    "time_steps = len(train_data)-1\n",
    "batch_size = 16\n",
    "hidden_size = max_nodes\n",
    "# train the lstm\n",
    "train_data_transposed = []\n",
    "for adj_mat in train_data:\n",
    "    train_data_transposed.append(np.transpose(adj_mat))\n",
    "train_data_seg = train_data[0:-1]\n",
    "target_seg = train_data[-1]\n",
    "train_batch_number = int(graphs_train_len/batch_size)\n",
    "train_data_np = np.array(train_data_seg)\n",
    "target_np = np.array(target_seg)\n",
    "many_one_model = nn.LSTM(max_nodes,hidden_size)\n",
    "while current_epoch < total_epoch:\n",
    "    for i in range(train_batch_number):        \n",
    "        input_data = train_data_np[:,i*train_batch_number:i*train_batch_number+batch_size,:]\n",
    "        target = target_np[i*train_batch_number:i*train_batch_number+batch_size,:]\n",
    "        input_data = Variable(torch.from_numpy(input_data).float())\n",
    "        target = Variable(torch.from_numpy(target).float())\n",
    "        predicted_output,_ = many_one_model(input_data)\n",
    "        last_output = predicted_output[-1,:,:]\n",
    "        print(last_output, ' ',target)    \n",
    "        error = nn.functional.binary_cross_entropy_with_logits(last_output,target)\n",
    "        error.backward()\n",
    "    current_epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # c = [3, 5, 7, 9]\n",
    "# # v=list(enumerate(c))\n",
    "# # random.shuffle(v)\n",
    "# # c_idx,c = zip(*v)\n",
    "# # print(c_idx,' ',c)\n",
    "# train_data_np = np.array(train_data)\n",
    "# train_sample = train_data_np[-1,:,:]\n",
    "# input_seq = Variable(torch.tensor(train_sample))\n",
    "# print(input_seq)\n",
    "time_steps = 10\n",
    "batch_size = 3\n",
    "in_size = 5\n",
    "classes_no = 7\n",
    "model = nn.LSTM(in_size,hidden_size = 1)\n",
    "input_seq = Variable(torch.randn(time_steps, batch_size, in_size))\n",
    "print(input_seq.size(1))\n",
    "# target = Variable(torch.LongTensor(batch_size).random_(0, 1))\n",
    "target = Variable(input_seq[-1])\n",
    "output_seq, _ = model(input_seq)\n",
    "last_output = output_seq[-1]\n",
    "print(last_output, ' ',target)\n",
    "err = nn.functional.binary_cross_entropy(last_output, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
