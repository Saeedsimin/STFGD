{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1756.0\n",
      "2631   659\n"
     ]
    }
   ],
   "source": [
    "# get the adjacency coordinates and turn them into matrix\n",
    "# get the idxs for adjacency (using id_inverse_map) and creat x, tx, y, ty, allx, ally, \n",
    "# make sure both test, train data comes with their id_map (loss of data in dynamic training)\n",
    "#### \n",
    "\"\"\"\n",
    "Loads input data from gcn/data directory\n",
    "\n",
    "ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "    (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "    object;\n",
    "ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "All objects above must be saved using python pickle module.\n",
    "\n",
    ":param dataset_str: Dataset name\n",
    ":return: All data input files loaded (as well the training/test data).\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "name = 'Amazon'\n",
    "path = './' + name\n",
    "max_nodes = 15\n",
    "\n",
    "\n",
    "\n",
    "id_file = open(path + '/Amazon_user_id_map.txt','r')\n",
    "id_map = json.load(id_file)\n",
    "\n",
    "# id_file_inverse = open(path + '/Amazon_user_id_map_inverse.txt', 'r')\n",
    "id_map_inverse = dict([(value, key) for key, value in id_map.items()]) \n",
    " \n",
    "graph_neighbor_file = open(path + '/group_user_tight_adj.json','r')\n",
    "graph_neighbor_dic = json.load(graph_neighbor_file)\n",
    "features = np.loadtxt(path + '/Amazon_node_attributes.txt',delimiter=',')\n",
    "adj_mat = np.loadtxt(path+ '/Amazon_A.txt',delimiter=',').astype(int)\n",
    "labels = np.loadtxt(path+ '/Amazon_node_labels.txt', delimiter=',').astype(int)\n",
    "graph_indicator = np.loadtxt(path+'/Amazon_graph_indicator.txt', delimiter=',').astype(int)\n",
    "graph_labels = np.loadtxt(path+'/Amazon_graph_labels.txt', delimiter=',').astype(int)\n",
    "data_tuple = list(map(tuple, adj_mat))\n",
    "graph_num = graph_indicator.max()\n",
    "\n",
    "group_list = [[] for _ in range(graph_num+1)]\n",
    "graph_idx = 0\n",
    "id_map_inverse_new = {}\n",
    "features_list = []\n",
    "labels_list = []\n",
    "labels2d_list = []\n",
    "for idx, feature  in enumerate(features):\n",
    "    user_temp = group_list[graph_indicator[idx]]\n",
    "    if len(user_temp) <= max_nodes:\n",
    "        graph_idx += 1\n",
    "        group_list[graph_indicator[int(idx)]].append(graph_idx)\n",
    "        id_map_inverse_new[graph_idx] = id_map_inverse[int(idx+1)]\n",
    "        features_list.append(feature)\n",
    "        labels_list.append(labels[idx])\n",
    "        label2d = [0 for _ in range(2)]\n",
    "        label2d[labels[idx]] = 1\n",
    "        labels2d_list.append(label2d)\n",
    "# create graph neighbor dic        \n",
    "id_map_new = {value:key for key, value in id_map_inverse_new.items()}\n",
    "graph_dic = {} #\n",
    "for key, value in graph_neighbor_dic.items():\n",
    "    if key in id_map_new:\n",
    "        temp = []\n",
    "        flag = False\n",
    "        for item in value:\n",
    "            if str(item) in id_map_new:\n",
    "                flag = True\n",
    "                temp.append(id_map_new[str(item)]-1)\n",
    "        if flag == True:            \n",
    "            graph_dic[int(id_map_new[key]-1)] = temp\n",
    "# create the user cut off map dic\n",
    "cut_off_idx = 0\n",
    "user_cut_off_map = {}\n",
    "for key, value in graph_dic.items():\n",
    "    if key not in user_cut_off_map:\n",
    "        user_cut_off_map[key] = cut_off_idx\n",
    "        cut_off_idx += 1\n",
    "    for user_id in value:\n",
    "        if user_id not in user_cut_off_map:\n",
    "            user_cut_off_map[user_id] = cut_off_idx\n",
    "            cut_off_idx += 1\n",
    "            \n",
    "# create features and labels list\n",
    "features_cut_off_list = [[] for _ in range(cut_off_idx)]\n",
    "labels_cut_off_list = [[] for _ in range(cut_off_idx)]\n",
    "for key, value in user_cut_off_map.items():\n",
    "    features_cut_off_list[value] = features_list[key]\n",
    "    labels_cut_off_list[value] = labels2d_list[key]\n",
    "# create graph neighbor cut off dic    \n",
    "graphs_cut_off_dic = {}            \n",
    "for key, value in graph_dic.items():\n",
    "    graphs_cut_off_dic[user_cut_off_map[key]] = []\n",
    "    for item in value:\n",
    "        graphs_cut_off_dic[user_cut_off_map[key]].append(user_cut_off_map[item])\n",
    "\n",
    "# doing graphs_cut_off_dic    \n",
    "graphs_len = len(group_list)\n",
    "print(0.8 * graphs_len, )\n",
    "graphs_test = group_list[int(0.8 * graphs_len):]\n",
    "graphs_train = group_list[0:int(0.8*graphs_len)]\n",
    "graphs_validate = group_list[0:int(0.2*graphs_len)]\n",
    "features_train = []\n",
    "labels_train = []\n",
    "for list_idx, node_list  in enumerate(graphs_train):\n",
    "    for node_idx, node_id  in enumerate(node_list):\n",
    "        if node_id-1 in user_cut_off_map:            \n",
    "            features_train.append(features_cut_off_list[user_cut_off_map[node_id-1]])\n",
    "            labels_train.append(labels_cut_off_list[user_cut_off_map[node_id-1]])\n",
    "\n",
    "features_test = []\n",
    "labels_test = []\n",
    "test_index = []\n",
    "for list_idx, node_list  in enumerate(graphs_test):\n",
    "    for node_idx, node_id  in enumerate(node_list):\n",
    "        if node_id-1 in user_cut_off_map:            \n",
    "            features_test.append(features_cut_off_list[user_cut_off_map[node_id-1]])\n",
    "            labels_test.append(labels_cut_off_list[user_cut_off_map[node_id-1]])\n",
    "            test_index.append(user_cut_off_map[node_id-1])\n",
    "print(len(features_train),' ', len(features_test))\n",
    "yelp_x = csr_matrix(np.array(features_train))\n",
    "yelp_tx = csr_matrix(np.array(features_test))\n",
    "yelp_allx = csr_matrix(np.array(features_cut_off_list))\n",
    "\n",
    "yelp_y = np.array(labels_train)\n",
    "yelp_ty = np.array(labels_test)\n",
    "yelp_ally = np.array(labels_cut_off_list)\n",
    "graph_dic = defaultdict(list, graphs_cut_off_dic)\n",
    "\n",
    "pkl.dump(yelp_x,open('ind.{}.x'.format(name),'wb'))\n",
    "pkl.dump(yelp_tx,open('ind.{}.tx'.format(name),'wb'))\n",
    "pkl.dump(yelp_x,open('ind.{}.allx'.format(name),'wb'))\n",
    "\n",
    "pkl.dump(yelp_y,open('ind.{}.y'.format(name),'wb'))\n",
    "pkl.dump(yelp_ty,open('ind.{}.ty'.format(name),'wb'))\n",
    "pkl.dump(yelp_y,open('ind.{}.ally'.format(name),'wb'))\n",
    "\n",
    "pkl.dump(graph_dic,open('ind.{}.graph'.format(name),'wb'))\n",
    "index_file = open('ind.{}.test.index'.format(name),'w')\n",
    "for item in test_index:    \n",
    "    index_file.write(str(item) + '\\n')\n",
    "index_file.close()\n",
    "# pkl.dump(test_index,open('ind.{}.test.index'.format(name),'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "dataset_str = 'Yelp'\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []\n",
    "for i in range(len(names)):\n",
    "    with open(\"ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "        if sys.version_info > (3, 0):\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "        else:\n",
    "            objects.append(pkl.load(f))\n",
    "            \n",
    "x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "test_idx_reorder = parse_index_file(\"ind.{}.test.index\".format(dataset_str))\n",
    "test_idx_range = np.sort(test_idx_reorder)\n",
    "features = sp.vstack((allx, tx)).tolil()\n",
    "features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "labels = np.vstack((ally, ty))\n",
    "labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "idx_test = test_idx_range.tolist()\n",
    "idx_train = range(len(y))\n",
    "idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "y_train = np.zeros(labels.shape)\n",
    "y_val = np.zeros(labels.shape)\n",
    "y_test = np.zeros(labels.shape)\n",
    "y_train[train_mask, :] = labels[train_mask, :]\n",
    "y_val[val_mask, :] = labels[val_mask, :]\n",
    "y_test[test_mask, :] = labels[test_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t4\n",
      "  (0, 2)\t3\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "sparse_matrix = scipy.sparse.csc_matrix(np.array([[0, 0, 3], [4, 0, 0]]))\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3cb088e2b33b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
