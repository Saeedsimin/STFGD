{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uniwa/students6/students/22458906/linux/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/uniwa/students6/students/22458906/linux/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.69543 train_acc= 0.43421 val_loss= 0.69530 val_acc= 0.30200 val_pre= 0.81579 time= 0.07619\n",
      "Epoch: 0002 train_loss= 0.69530 train_acc= 0.49132 val_loss= 0.69518 val_acc= 0.35400 val_pre= 0.83562 time= 0.01675\n",
      "Epoch: 0003 train_loss= 0.69517 train_acc= 0.52193 val_loss= 0.69506 val_acc= 0.44200 val_pre= 0.85149 time= 0.01582\n",
      "Epoch: 0004 train_loss= 0.69505 train_acc= 0.54209 val_loss= 0.69494 val_acc= 0.59000 val_pre= 0.85065 time= 0.01575\n",
      "Epoch: 0005 train_loss= 0.69494 train_acc= 0.60181 val_loss= 0.69483 val_acc= 0.69600 val_pre= 0.83879 time= 0.01556\n",
      "Epoch: 0006 train_loss= 0.69482 train_acc= 0.63410 val_loss= 0.69472 val_acc= 0.78200 val_pre= 0.84211 time= 0.01492\n",
      "Epoch: 0007 train_loss= 0.69472 train_acc= 0.68141 val_loss= 0.69462 val_acc= 0.82800 val_pre= 0.84536 time= 0.01723\n",
      "Epoch: 0008 train_loss= 0.69462 train_acc= 0.71930 val_loss= 0.69453 val_acc= 0.83800 val_pre= 0.84694 time= 0.01701\n",
      "Epoch: 0009 train_loss= 0.69452 train_acc= 0.74963 val_loss= 0.69444 val_acc= 0.84000 val_pre= 0.84444 time= 0.01605\n",
      "Epoch: 0010 train_loss= 0.69443 train_acc= 0.78070 val_loss= 0.69435 val_acc= 0.84400 val_pre= 0.84507 time= 0.01642\n",
      "Epoch: 0011 train_loss= 0.69434 train_acc= 0.79955 val_loss= 0.69427 val_acc= 0.84200 val_pre= 0.84200 time= 0.01981\n",
      "Epoch: 0012 train_loss= 0.69426 train_acc= 0.81915 val_loss= 0.69419 val_acc= 0.84200 val_pre= 0.84200 time= 0.02147\n",
      "Epoch: 0013 train_loss= 0.69418 train_acc= 0.82932 val_loss= 0.69412 val_acc= 0.84200 val_pre= 0.84200 time= 0.02350\n",
      "Epoch: 0014 train_loss= 0.69411 train_acc= 0.84033 val_loss= 0.69405 val_acc= 0.84200 val_pre= 0.84200 time= 0.02308\n",
      "Epoch: 0015 train_loss= 0.69404 train_acc= 0.84911 val_loss= 0.69398 val_acc= 0.84200 val_pre= 0.84200 time= 0.02151\n",
      "Epoch: 0016 train_loss= 0.69397 train_acc= 0.85862 val_loss= 0.69392 val_acc= 0.84200 val_pre= 0.84200 time= 0.02085\n",
      "Epoch: 0017 train_loss= 0.69391 train_acc= 0.86516 val_loss= 0.69386 val_acc= 0.84200 val_pre= 0.84200 time= 0.02124\n",
      "Epoch: 0018 train_loss= 0.69385 train_acc= 0.87029 val_loss= 0.69381 val_acc= 0.84200 val_pre= 0.84200 time= 0.01653\n",
      "Epoch: 0019 train_loss= 0.69380 train_acc= 0.87272 val_loss= 0.69376 val_acc= 0.84200 val_pre= 0.84200 time= 0.01699\n",
      "Epoch: 0020 train_loss= 0.69375 train_acc= 0.87617 val_loss= 0.69371 val_acc= 0.84200 val_pre= 0.84200 time= 0.01640\n",
      "Epoch: 0021 train_loss= 0.69370 train_acc= 0.87757 val_loss= 0.69366 val_acc= 0.84200 val_pre= 0.84200 time= 0.01681\n",
      "Epoch: 0022 train_loss= 0.69365 train_acc= 0.87691 val_loss= 0.69362 val_acc= 0.84200 val_pre= 0.84200 time= 0.01671\n",
      "Epoch: 0023 train_loss= 0.69361 train_acc= 0.87850 val_loss= 0.69358 val_acc= 0.84200 val_pre= 0.84200 time= 0.01623\n",
      "Epoch: 0024 train_loss= 0.69357 train_acc= 0.87999 val_loss= 0.69355 val_acc= 0.84200 val_pre= 0.84200 time= 0.01760\n",
      "Epoch: 0025 train_loss= 0.69354 train_acc= 0.88065 val_loss= 0.69351 val_acc= 0.84200 val_pre= 0.84200 time= 0.01824\n",
      "Epoch: 0026 train_loss= 0.69350 train_acc= 0.88093 val_loss= 0.69348 val_acc= 0.84200 val_pre= 0.84200 time= 0.01687\n",
      "Epoch: 0027 train_loss= 0.69347 train_acc= 0.88345 val_loss= 0.69345 val_acc= 0.84200 val_pre= 0.84200 time= 0.01637\n",
      "Epoch: 0028 train_loss= 0.69344 train_acc= 0.88233 val_loss= 0.69342 val_acc= 0.84200 val_pre= 0.84200 time= 0.01724\n",
      "Epoch: 0029 train_loss= 0.69341 train_acc= 0.88251 val_loss= 0.69340 val_acc= 0.84200 val_pre= 0.84200 time= 0.01796\n",
      "Epoch: 0030 train_loss= 0.69339 train_acc= 0.88149 val_loss= 0.69337 val_acc= 0.84200 val_pre= 0.84200 time= 0.01876\n",
      "Epoch: 0031 train_loss= 0.69336 train_acc= 0.88214 val_loss= 0.69335 val_acc= 0.84200 val_pre= 0.84200 time= 0.01538\n",
      "Epoch: 0032 train_loss= 0.69334 train_acc= 0.88317 val_loss= 0.69333 val_acc= 0.84200 val_pre= 0.84200 time= 0.01546\n",
      "Epoch: 0033 train_loss= 0.69332 train_acc= 0.88373 val_loss= 0.69331 val_acc= 0.84200 val_pre= 0.84200 time= 0.01498\n",
      "Epoch: 0034 train_loss= 0.69330 train_acc= 0.88419 val_loss= 0.69330 val_acc= 0.84200 val_pre= 0.84200 time= 0.01683\n",
      "Epoch: 0035 train_loss= 0.69329 train_acc= 0.88438 val_loss= 0.69328 val_acc= 0.84200 val_pre= 0.84200 time= 0.01456\n",
      "Epoch: 0036 train_loss= 0.69327 train_acc= 0.88457 val_loss= 0.69327 val_acc= 0.84200 val_pre= 0.84200 time= 0.01736\n",
      "Epoch: 0037 train_loss= 0.69326 train_acc= 0.88457 val_loss= 0.69325 val_acc= 0.84200 val_pre= 0.84200 time= 0.01814\n",
      "Epoch: 0038 train_loss= 0.69324 train_acc= 0.88429 val_loss= 0.69324 val_acc= 0.84200 val_pre= 0.84200 time= 0.01519\n",
      "Epoch: 0039 train_loss= 0.69323 train_acc= 0.88457 val_loss= 0.69323 val_acc= 0.84200 val_pre= 0.84200 time= 0.01641\n",
      "Epoch: 0040 train_loss= 0.69322 train_acc= 0.88457 val_loss= 0.69322 val_acc= 0.84200 val_pre= 0.84200 time= 0.01561\n",
      "Epoch: 0041 train_loss= 0.69321 train_acc= 0.88457 val_loss= 0.69321 val_acc= 0.84200 val_pre= 0.84200 time= 0.01556\n",
      "Epoch: 0042 train_loss= 0.69320 train_acc= 0.88457 val_loss= 0.69320 val_acc= 0.84200 val_pre= 0.84200 time= 0.01546\n",
      "Epoch: 0043 train_loss= 0.69319 train_acc= 0.88457 val_loss= 0.69319 val_acc= 0.84200 val_pre= 0.84200 time= 0.01451\n",
      "Epoch: 0044 train_loss= 0.69318 train_acc= 0.88457 val_loss= 0.69318 val_acc= 0.84200 val_pre= 0.84200 time= 0.01677\n",
      "Epoch: 0045 train_loss= 0.69317 train_acc= 0.88457 val_loss= 0.69317 val_acc= 0.84200 val_pre= 0.84200 time= 0.01660\n",
      "Epoch: 0046 train_loss= 0.69316 train_acc= 0.88457 val_loss= 0.69317 val_acc= 0.84200 val_pre= 0.84200 time= 0.01490\n",
      "Epoch: 0047 train_loss= 0.69316 train_acc= 0.88457 val_loss= 0.69316 val_acc= 0.84200 val_pre= 0.84200 time= 0.01530\n",
      "Epoch: 0048 train_loss= 0.69315 train_acc= 0.88457 val_loss= 0.69315 val_acc= 0.84200 val_pre= 0.84200 time= 0.01476\n",
      "Epoch: 0049 train_loss= 0.69314 train_acc= 0.88457 val_loss= 0.69315 val_acc= 0.84200 val_pre= 0.84200 time= 0.01473\n",
      "Epoch: 0050 train_loss= 0.69314 train_acc= 0.88457 val_loss= 0.69314 val_acc= 0.84200 val_pre= 0.84200 time= 0.01479\n",
      "Epoch: 0051 train_loss= 0.69313 train_acc= 0.88457 val_loss= 0.69314 val_acc= 0.84200 val_pre= 0.84200 time= 0.01523\n",
      "Epoch: 0052 train_loss= 0.69312 train_acc= 0.88457 val_loss= 0.69313 val_acc= 0.84200 val_pre= 0.84200 time= 0.01474\n",
      "Epoch: 0053 train_loss= 0.69312 train_acc= 0.88457 val_loss= 0.69313 val_acc= 0.84200 val_pre= 0.84200 time= 0.01442\n",
      "Epoch: 0054 train_loss= 0.69311 train_acc= 0.88457 val_loss= 0.69312 val_acc= 0.84200 val_pre= 0.84200 time= 0.01470\n",
      "Epoch: 0055 train_loss= 0.69311 train_acc= 0.88457 val_loss= 0.69312 val_acc= 0.84200 val_pre= 0.84200 time= 0.01502\n",
      "Epoch: 0056 train_loss= 0.69310 train_acc= 0.88457 val_loss= 0.69311 val_acc= 0.84200 val_pre= 0.84200 time= 0.01593\n",
      "Epoch: 0057 train_loss= 0.69310 train_acc= 0.88457 val_loss= 0.69311 val_acc= 0.84200 val_pre= 0.84200 time= 0.01519\n",
      "Epoch: 0058 train_loss= 0.69309 train_acc= 0.88457 val_loss= 0.69311 val_acc= 0.84200 val_pre= 0.84200 time= 0.01579\n",
      "Epoch: 0059 train_loss= 0.69309 train_acc= 0.88457 val_loss= 0.69310 val_acc= 0.84200 val_pre= 0.84200 time= 0.01678\n",
      "Epoch: 0060 train_loss= 0.69308 train_acc= 0.88457 val_loss= 0.69310 val_acc= 0.84200 val_pre= 0.84200 time= 0.01669\n",
      "Epoch: 0061 train_loss= 0.69308 train_acc= 0.88457 val_loss= 0.69309 val_acc= 0.84200 val_pre= 0.84200 time= 0.01494\n",
      "Epoch: 0062 train_loss= 0.69307 train_acc= 0.88457 val_loss= 0.69309 val_acc= 0.84200 val_pre= 0.84200 time= 0.01597\n",
      "Epoch: 0063 train_loss= 0.69307 train_acc= 0.88457 val_loss= 0.69309 val_acc= 0.84200 val_pre= 0.84200 time= 0.01817\n",
      "Epoch: 0064 train_loss= 0.69306 train_acc= 0.88457 val_loss= 0.69308 val_acc= 0.84200 val_pre= 0.84200 time= 0.01587\n",
      "Epoch: 0065 train_loss= 0.69306 train_acc= 0.88457 val_loss= 0.69308 val_acc= 0.84200 val_pre= 0.84200 time= 0.01839\n",
      "Epoch: 0066 train_loss= 0.69305 train_acc= 0.88457 val_loss= 0.69307 val_acc= 0.84200 val_pre= 0.84200 time= 0.01589\n",
      "Epoch: 0067 train_loss= 0.69305 train_acc= 0.88457 val_loss= 0.69307 val_acc= 0.84200 val_pre= 0.84200 time= 0.01480\n",
      "Epoch: 0068 train_loss= 0.69304 train_acc= 0.88457 val_loss= 0.69307 val_acc= 0.84200 val_pre= 0.84200 time= 0.01523\n",
      "Epoch: 0069 train_loss= 0.69304 train_acc= 0.88457 val_loss= 0.69306 val_acc= 0.84200 val_pre= 0.84200 time= 0.01476\n",
      "Epoch: 0070 train_loss= 0.69303 train_acc= 0.88457 val_loss= 0.69306 val_acc= 0.84200 val_pre= 0.84200 time= 0.01443\n",
      "Epoch: 0071 train_loss= 0.69302 train_acc= 0.88457 val_loss= 0.69306 val_acc= 0.84200 val_pre= 0.84200 time= 0.01431\n",
      "Epoch: 0072 train_loss= 0.69302 train_acc= 0.88457 val_loss= 0.69305 val_acc= 0.84200 val_pre= 0.84200 time= 0.01455\n",
      "Epoch: 0073 train_loss= 0.69301 train_acc= 0.88457 val_loss= 0.69305 val_acc= 0.84200 val_pre= 0.84200 time= 0.01411\n",
      "Epoch: 0074 train_loss= 0.69301 train_acc= 0.88457 val_loss= 0.69304 val_acc= 0.84200 val_pre= 0.84200 time= 0.01360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0075 train_loss= 0.69300 train_acc= 0.88457 val_loss= 0.69304 val_acc= 0.84200 val_pre= 0.84200 time= 0.01487\n",
      "Epoch: 0076 train_loss= 0.69299 train_acc= 0.88457 val_loss= 0.69303 val_acc= 0.84200 val_pre= 0.84200 time= 0.01438\n",
      "Epoch: 0077 train_loss= 0.69299 train_acc= 0.88457 val_loss= 0.69303 val_acc= 0.84200 val_pre= 0.84200 time= 0.01449\n",
      "Epoch: 0078 train_loss= 0.69299 train_acc= 0.88457 val_loss= 0.69303 val_acc= 0.84200 val_pre= 0.84200 time= 0.01501\n",
      "Epoch: 0079 train_loss= 0.69298 train_acc= 0.88457 val_loss= 0.69302 val_acc= 0.84200 val_pre= 0.84200 time= 0.01430\n",
      "Epoch: 0080 train_loss= 0.69297 train_acc= 0.88457 val_loss= 0.69302 val_acc= 0.84200 val_pre= 0.84200 time= 0.01364\n",
      "Epoch: 0081 train_loss= 0.69296 train_acc= 0.88457 val_loss= 0.69301 val_acc= 0.84200 val_pre= 0.84200 time= 0.01446\n",
      "Epoch: 0082 train_loss= 0.69296 train_acc= 0.88457 val_loss= 0.69301 val_acc= 0.84200 val_pre= 0.84200 time= 0.01431\n",
      "Epoch: 0083 train_loss= 0.69295 train_acc= 0.88457 val_loss= 0.69300 val_acc= 0.84200 val_pre= 0.84200 time= 0.01375\n",
      "Epoch: 0084 train_loss= 0.69294 train_acc= 0.88457 val_loss= 0.69300 val_acc= 0.84200 val_pre= 0.84200 time= 0.01428\n",
      "Epoch: 0085 train_loss= 0.69294 train_acc= 0.88457 val_loss= 0.69299 val_acc= 0.84200 val_pre= 0.84200 time= 0.01365\n",
      "Epoch: 0086 train_loss= 0.69292 train_acc= 0.88457 val_loss= 0.69299 val_acc= 0.84200 val_pre= 0.84200 time= 0.01409\n",
      "Epoch: 0087 train_loss= 0.69292 train_acc= 0.88457 val_loss= 0.69298 val_acc= 0.84200 val_pre= 0.84200 time= 0.01431\n",
      "Epoch: 0088 train_loss= 0.69291 train_acc= 0.88457 val_loss= 0.69297 val_acc= 0.84200 val_pre= 0.84200 time= 0.01478\n",
      "Epoch: 0089 train_loss= 0.69291 train_acc= 0.88457 val_loss= 0.69297 val_acc= 0.84200 val_pre= 0.84200 time= 0.01481\n",
      "Epoch: 0090 train_loss= 0.69289 train_acc= 0.88457 val_loss= 0.69296 val_acc= 0.84200 val_pre= 0.84200 time= 0.01504\n",
      "Epoch: 0091 train_loss= 0.69289 train_acc= 0.88457 val_loss= 0.69296 val_acc= 0.84200 val_pre= 0.84200 time= 0.01441\n",
      "Epoch: 0092 train_loss= 0.69288 train_acc= 0.88457 val_loss= 0.69295 val_acc= 0.84200 val_pre= 0.84200 time= 0.01488\n",
      "Epoch: 0093 train_loss= 0.69287 train_acc= 0.88457 val_loss= 0.69294 val_acc= 0.84200 val_pre= 0.84200 time= 0.01454\n",
      "Epoch: 0094 train_loss= 0.69286 train_acc= 0.88457 val_loss= 0.69294 val_acc= 0.84200 val_pre= 0.84200 time= 0.01448\n",
      "Epoch: 0095 train_loss= 0.69285 train_acc= 0.88457 val_loss= 0.69293 val_acc= 0.84200 val_pre= 0.84200 time= 0.01559\n",
      "Epoch: 0096 train_loss= 0.69284 train_acc= 0.88457 val_loss= 0.69292 val_acc= 0.84200 val_pre= 0.84200 time= 0.01421\n",
      "Epoch: 0097 train_loss= 0.69283 train_acc= 0.88457 val_loss= 0.69292 val_acc= 0.84200 val_pre= 0.84200 time= 0.01385\n",
      "Epoch: 0098 train_loss= 0.69282 train_acc= 0.88457 val_loss= 0.69291 val_acc= 0.84200 val_pre= 0.84200 time= 0.01409\n",
      "Epoch: 0099 train_loss= 0.69281 train_acc= 0.88457 val_loss= 0.69290 val_acc= 0.84200 val_pre= 0.84200 time= 0.01494\n",
      "Epoch: 0100 train_loss= 0.69279 train_acc= 0.88457 val_loss= 0.69290 val_acc= 0.84200 val_pre= 0.84200 time= 0.01483\n",
      "Epoch: 0101 train_loss= 0.69279 train_acc= 0.88457 val_loss= 0.69289 val_acc= 0.84200 val_pre= 0.84200 time= 0.01424\n",
      "Epoch: 0102 train_loss= 0.69278 train_acc= 0.88457 val_loss= 0.69288 val_acc= 0.84200 val_pre= 0.84200 time= 0.01409\n",
      "Epoch: 0103 train_loss= 0.69277 train_acc= 0.88457 val_loss= 0.69287 val_acc= 0.84200 val_pre= 0.84200 time= 0.01402\n",
      "Epoch: 0104 train_loss= 0.69276 train_acc= 0.88457 val_loss= 0.69287 val_acc= 0.84200 val_pre= 0.84200 time= 0.01430\n",
      "Epoch: 0105 train_loss= 0.69275 train_acc= 0.88457 val_loss= 0.69286 val_acc= 0.84200 val_pre= 0.84200 time= 0.01483\n",
      "Epoch: 0106 train_loss= 0.69273 train_acc= 0.88457 val_loss= 0.69285 val_acc= 0.84200 val_pre= 0.84200 time= 0.01406\n",
      "Epoch: 0107 train_loss= 0.69272 train_acc= 0.88457 val_loss= 0.69284 val_acc= 0.84200 val_pre= 0.84200 time= 0.01483\n",
      "Epoch: 0108 train_loss= 0.69271 train_acc= 0.88457 val_loss= 0.69283 val_acc= 0.84200 val_pre= 0.84200 time= 0.01543\n",
      "Epoch: 0109 train_loss= 0.69270 train_acc= 0.88457 val_loss= 0.69282 val_acc= 0.84200 val_pre= 0.84200 time= 0.01522\n",
      "Epoch: 0110 train_loss= 0.69270 train_acc= 0.88457 val_loss= 0.69282 val_acc= 0.84200 val_pre= 0.84200 time= 0.01413\n",
      "Epoch: 0111 train_loss= 0.69267 train_acc= 0.88457 val_loss= 0.69281 val_acc= 0.84200 val_pre= 0.84200 time= 0.01440\n",
      "Epoch: 0112 train_loss= 0.69266 train_acc= 0.88457 val_loss= 0.69280 val_acc= 0.84200 val_pre= 0.84200 time= 0.01407\n",
      "Epoch: 0113 train_loss= 0.69265 train_acc= 0.88457 val_loss= 0.69279 val_acc= 0.84200 val_pre= 0.84200 time= 0.01376\n",
      "Epoch: 0114 train_loss= 0.69264 train_acc= 0.88457 val_loss= 0.69278 val_acc= 0.84200 val_pre= 0.84200 time= 0.01480\n",
      "Epoch: 0115 train_loss= 0.69262 train_acc= 0.88457 val_loss= 0.69277 val_acc= 0.84200 val_pre= 0.84200 time= 0.01513\n",
      "Epoch: 0116 train_loss= 0.69261 train_acc= 0.88457 val_loss= 0.69276 val_acc= 0.84200 val_pre= 0.84200 time= 0.01669\n",
      "Epoch: 0117 train_loss= 0.69260 train_acc= 0.88457 val_loss= 0.69275 val_acc= 0.84200 val_pre= 0.84200 time= 0.01781\n",
      "Epoch: 0118 train_loss= 0.69258 train_acc= 0.88457 val_loss= 0.69274 val_acc= 0.84200 val_pre= 0.84200 time= 0.01796\n",
      "Epoch: 0119 train_loss= 0.69258 train_acc= 0.88457 val_loss= 0.69273 val_acc= 0.84200 val_pre= 0.84200 time= 0.01785\n",
      "Epoch: 0120 train_loss= 0.69254 train_acc= 0.88457 val_loss= 0.69272 val_acc= 0.84200 val_pre= 0.84200 time= 0.01736\n",
      "Epoch: 0121 train_loss= 0.69254 train_acc= 0.88457 val_loss= 0.69271 val_acc= 0.84200 val_pre= 0.84200 time= 0.01748\n",
      "Epoch: 0122 train_loss= 0.69251 train_acc= 0.88457 val_loss= 0.69270 val_acc= 0.84200 val_pre= 0.84200 time= 0.01780\n",
      "Epoch: 0123 train_loss= 0.69250 train_acc= 0.88457 val_loss= 0.69269 val_acc= 0.84200 val_pre= 0.84200 time= 0.01813\n",
      "Epoch: 0124 train_loss= 0.69250 train_acc= 0.88457 val_loss= 0.69268 val_acc= 0.84200 val_pre= 0.84200 time= 0.01748\n",
      "Epoch: 0125 train_loss= 0.69246 train_acc= 0.88457 val_loss= 0.69267 val_acc= 0.84200 val_pre= 0.84200 time= 0.01745\n",
      "Epoch: 0126 train_loss= 0.69247 train_acc= 0.88457 val_loss= 0.69266 val_acc= 0.84200 val_pre= 0.84200 time= 0.01856\n",
      "Epoch: 0127 train_loss= 0.69245 train_acc= 0.88457 val_loss= 0.69265 val_acc= 0.84200 val_pre= 0.84200 time= 0.02133\n",
      "Epoch: 0128 train_loss= 0.69243 train_acc= 0.88457 val_loss= 0.69263 val_acc= 0.84200 val_pre= 0.84200 time= 0.02092\n",
      "Epoch: 0129 train_loss= 0.69240 train_acc= 0.88457 val_loss= 0.69262 val_acc= 0.84200 val_pre= 0.84200 time= 0.02175\n",
      "Epoch: 0130 train_loss= 0.69240 train_acc= 0.88457 val_loss= 0.69261 val_acc= 0.84200 val_pre= 0.84200 time= 0.02132\n",
      "Epoch: 0131 train_loss= 0.69238 train_acc= 0.88457 val_loss= 0.69260 val_acc= 0.84200 val_pre= 0.84200 time= 0.02148\n",
      "Epoch: 0132 train_loss= 0.69236 train_acc= 0.88457 val_loss= 0.69259 val_acc= 0.84200 val_pre= 0.84200 time= 0.02044\n",
      "Epoch: 0133 train_loss= 0.69233 train_acc= 0.88457 val_loss= 0.69258 val_acc= 0.84200 val_pre= 0.84200 time= 0.02075\n",
      "Epoch: 0134 train_loss= 0.69234 train_acc= 0.88457 val_loss= 0.69256 val_acc= 0.84200 val_pre= 0.84200 time= 0.02048\n",
      "Epoch: 0135 train_loss= 0.69231 train_acc= 0.88457 val_loss= 0.69255 val_acc= 0.84200 val_pre= 0.84200 time= 0.02139\n",
      "Epoch: 0136 train_loss= 0.69228 train_acc= 0.88457 val_loss= 0.69254 val_acc= 0.84200 val_pre= 0.84200 time= 0.02091\n",
      "Epoch: 0137 train_loss= 0.69225 train_acc= 0.88457 val_loss= 0.69253 val_acc= 0.84200 val_pre= 0.84200 time= 0.02053\n",
      "Epoch: 0138 train_loss= 0.69224 train_acc= 0.88457 val_loss= 0.69251 val_acc= 0.84200 val_pre= 0.84200 time= 0.02178\n",
      "Epoch: 0139 train_loss= 0.69223 train_acc= 0.88457 val_loss= 0.69250 val_acc= 0.84200 val_pre= 0.84200 time= 0.02266\n",
      "Epoch: 0140 train_loss= 0.69221 train_acc= 0.88457 val_loss= 0.69249 val_acc= 0.84200 val_pre= 0.84200 time= 0.02154\n",
      "Epoch: 0141 train_loss= 0.69219 train_acc= 0.88457 val_loss= 0.69247 val_acc= 0.84200 val_pre= 0.84200 time= 0.02388\n",
      "Epoch: 0142 train_loss= 0.69217 train_acc= 0.88457 val_loss= 0.69246 val_acc= 0.84200 val_pre= 0.84200 time= 0.02060\n",
      "Epoch: 0143 train_loss= 0.69216 train_acc= 0.88457 val_loss= 0.69245 val_acc= 0.84200 val_pre= 0.84200 time= 0.02102\n",
      "Epoch: 0144 train_loss= 0.69215 train_acc= 0.88457 val_loss= 0.69243 val_acc= 0.84200 val_pre= 0.84200 time= 0.02074\n",
      "Epoch: 0145 train_loss= 0.69211 train_acc= 0.88457 val_loss= 0.69242 val_acc= 0.84200 val_pre= 0.84200 time= 0.02061\n",
      "Epoch: 0146 train_loss= 0.69208 train_acc= 0.88457 val_loss= 0.69241 val_acc= 0.84200 val_pre= 0.84200 time= 0.02146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0147 train_loss= 0.69208 train_acc= 0.88457 val_loss= 0.69239 val_acc= 0.84200 val_pre= 0.84200 time= 0.02086\n",
      "Epoch: 0148 train_loss= 0.69204 train_acc= 0.88457 val_loss= 0.69238 val_acc= 0.84200 val_pre= 0.84200 time= 0.02079\n",
      "Epoch: 0149 train_loss= 0.69204 train_acc= 0.88457 val_loss= 0.69236 val_acc= 0.84200 val_pre= 0.84200 time= 0.02148\n",
      "Epoch: 0150 train_loss= 0.69199 train_acc= 0.88457 val_loss= 0.69235 val_acc= 0.84200 val_pre= 0.84200 time= 0.02179\n",
      "Epoch: 0151 train_loss= 0.69197 train_acc= 0.88457 val_loss= 0.69234 val_acc= 0.84200 val_pre= 0.84200 time= 0.02187\n",
      "Epoch: 0152 train_loss= 0.69197 train_acc= 0.88457 val_loss= 0.69232 val_acc= 0.84200 val_pre= 0.84200 time= 0.02180\n",
      "Epoch: 0153 train_loss= 0.69194 train_acc= 0.88457 val_loss= 0.69231 val_acc= 0.84200 val_pre= 0.84200 time= 0.01808\n",
      "Epoch: 0154 train_loss= 0.69191 train_acc= 0.88457 val_loss= 0.69229 val_acc= 0.84200 val_pre= 0.84200 time= 0.01827\n",
      "Epoch: 0155 train_loss= 0.69190 train_acc= 0.88457 val_loss= 0.69228 val_acc= 0.84200 val_pre= 0.84200 time= 0.01879\n",
      "Epoch: 0156 train_loss= 0.69188 train_acc= 0.88457 val_loss= 0.69226 val_acc= 0.84200 val_pre= 0.84200 time= 0.01873\n",
      "Epoch: 0157 train_loss= 0.69189 train_acc= 0.88457 val_loss= 0.69225 val_acc= 0.84200 val_pre= 0.84200 time= 0.01915\n",
      "Epoch: 0158 train_loss= 0.69184 train_acc= 0.88457 val_loss= 0.69223 val_acc= 0.84200 val_pre= 0.84200 time= 0.02069\n",
      "Epoch: 0159 train_loss= 0.69182 train_acc= 0.88457 val_loss= 0.69221 val_acc= 0.84200 val_pre= 0.84200 time= 0.01961\n",
      "Epoch: 0160 train_loss= 0.69180 train_acc= 0.88457 val_loss= 0.69220 val_acc= 0.84200 val_pre= 0.84200 time= 0.02086\n",
      "Epoch: 0161 train_loss= 0.69176 train_acc= 0.88457 val_loss= 0.69218 val_acc= 0.84200 val_pre= 0.84200 time= 0.02158\n",
      "Epoch: 0162 train_loss= 0.69172 train_acc= 0.88457 val_loss= 0.69217 val_acc= 0.84200 val_pre= 0.84200 time= 0.02065\n",
      "Epoch: 0163 train_loss= 0.69174 train_acc= 0.88457 val_loss= 0.69215 val_acc= 0.84200 val_pre= 0.84200 time= 0.01950\n",
      "Epoch: 0164 train_loss= 0.69169 train_acc= 0.88457 val_loss= 0.69213 val_acc= 0.84200 val_pre= 0.84200 time= 0.02021\n",
      "Epoch: 0165 train_loss= 0.69170 train_acc= 0.88457 val_loss= 0.69212 val_acc= 0.84200 val_pre= 0.84200 time= 0.01981\n",
      "Epoch: 0166 train_loss= 0.69167 train_acc= 0.88457 val_loss= 0.69210 val_acc= 0.84200 val_pre= 0.84200 time= 0.02043\n",
      "Epoch: 0167 train_loss= 0.69159 train_acc= 0.88457 val_loss= 0.69208 val_acc= 0.84200 val_pre= 0.84200 time= 0.02033\n",
      "Epoch: 0168 train_loss= 0.69156 train_acc= 0.88457 val_loss= 0.69207 val_acc= 0.84200 val_pre= 0.84200 time= 0.02026\n",
      "Epoch: 0169 train_loss= 0.69157 train_acc= 0.88457 val_loss= 0.69205 val_acc= 0.84200 val_pre= 0.84200 time= 0.01999\n",
      "Epoch: 0170 train_loss= 0.69153 train_acc= 0.88457 val_loss= 0.69203 val_acc= 0.84200 val_pre= 0.84200 time= 0.02005\n",
      "Epoch: 0171 train_loss= 0.69150 train_acc= 0.88457 val_loss= 0.69202 val_acc= 0.84200 val_pre= 0.84200 time= 0.01986\n",
      "Epoch: 0172 train_loss= 0.69153 train_acc= 0.88457 val_loss= 0.69200 val_acc= 0.84200 val_pre= 0.84200 time= 0.01970\n",
      "Epoch: 0173 train_loss= 0.69147 train_acc= 0.88457 val_loss= 0.69198 val_acc= 0.84200 val_pre= 0.84200 time= 0.02009\n",
      "Epoch: 0174 train_loss= 0.69143 train_acc= 0.88457 val_loss= 0.69197 val_acc= 0.84200 val_pre= 0.84200 time= 0.01978\n",
      "Epoch: 0175 train_loss= 0.69141 train_acc= 0.88457 val_loss= 0.69195 val_acc= 0.84200 val_pre= 0.84200 time= 0.01975\n",
      "Epoch: 0176 train_loss= 0.69141 train_acc= 0.88457 val_loss= 0.69193 val_acc= 0.84200 val_pre= 0.84200 time= 0.01987\n",
      "Epoch: 0177 train_loss= 0.69135 train_acc= 0.88457 val_loss= 0.69191 val_acc= 0.84200 val_pre= 0.84200 time= 0.02035\n",
      "Epoch: 0178 train_loss= 0.69136 train_acc= 0.88457 val_loss= 0.69189 val_acc= 0.84200 val_pre= 0.84200 time= 0.02217\n",
      "Epoch: 0179 train_loss= 0.69131 train_acc= 0.88457 val_loss= 0.69188 val_acc= 0.84200 val_pre= 0.84200 time= 0.02233\n",
      "Epoch: 0180 train_loss= 0.69130 train_acc= 0.88457 val_loss= 0.69186 val_acc= 0.84200 val_pre= 0.84200 time= 0.02081\n",
      "Epoch: 0181 train_loss= 0.69122 train_acc= 0.88457 val_loss= 0.69184 val_acc= 0.84200 val_pre= 0.84200 time= 0.02061\n",
      "Epoch: 0182 train_loss= 0.69123 train_acc= 0.88457 val_loss= 0.69182 val_acc= 0.84200 val_pre= 0.84200 time= 0.01980\n",
      "Epoch: 0183 train_loss= 0.69121 train_acc= 0.88457 val_loss= 0.69180 val_acc= 0.84200 val_pre= 0.84200 time= 0.02021\n",
      "Epoch: 0184 train_loss= 0.69115 train_acc= 0.88457 val_loss= 0.69178 val_acc= 0.84200 val_pre= 0.84200 time= 0.01661\n",
      "Epoch: 0185 train_loss= 0.69115 train_acc= 0.88457 val_loss= 0.69176 val_acc= 0.84200 val_pre= 0.84200 time= 0.01668\n",
      "Epoch: 0186 train_loss= 0.69113 train_acc= 0.88457 val_loss= 0.69175 val_acc= 0.84200 val_pre= 0.84200 time= 0.01609\n",
      "Epoch: 0187 train_loss= 0.69105 train_acc= 0.88457 val_loss= 0.69173 val_acc= 0.84200 val_pre= 0.84200 time= 0.01610\n",
      "Epoch: 0188 train_loss= 0.69107 train_acc= 0.88457 val_loss= 0.69171 val_acc= 0.84200 val_pre= 0.84200 time= 0.01636\n",
      "Epoch: 0189 train_loss= 0.69105 train_acc= 0.88457 val_loss= 0.69169 val_acc= 0.84200 val_pre= 0.84200 time= 0.01657\n",
      "Epoch: 0190 train_loss= 0.69105 train_acc= 0.88457 val_loss= 0.69167 val_acc= 0.84200 val_pre= 0.84200 time= 0.01674\n",
      "Epoch: 0191 train_loss= 0.69100 train_acc= 0.88457 val_loss= 0.69165 val_acc= 0.84200 val_pre= 0.84200 time= 0.01492\n",
      "Epoch: 0192 train_loss= 0.69096 train_acc= 0.88457 val_loss= 0.69163 val_acc= 0.84200 val_pre= 0.84200 time= 0.01473\n",
      "Epoch: 0193 train_loss= 0.69091 train_acc= 0.88457 val_loss= 0.69161 val_acc= 0.84200 val_pre= 0.84200 time= 0.01540\n",
      "Epoch: 0194 train_loss= 0.69088 train_acc= 0.88457 val_loss= 0.69159 val_acc= 0.84200 val_pre= 0.84200 time= 0.01511\n",
      "Epoch: 0195 train_loss= 0.69083 train_acc= 0.88457 val_loss= 0.69157 val_acc= 0.84200 val_pre= 0.84200 time= 0.01432\n",
      "Epoch: 0196 train_loss= 0.69086 train_acc= 0.88457 val_loss= 0.69155 val_acc= 0.84200 val_pre= 0.84200 time= 0.01429\n",
      "Epoch: 0197 train_loss= 0.69077 train_acc= 0.88457 val_loss= 0.69153 val_acc= 0.84200 val_pre= 0.84200 time= 0.01478\n",
      "Epoch: 0198 train_loss= 0.69075 train_acc= 0.88457 val_loss= 0.69151 val_acc= 0.84200 val_pre= 0.84200 time= 0.01507\n",
      "Epoch: 0199 train_loss= 0.69075 train_acc= 0.88457 val_loss= 0.69149 val_acc= 0.84200 val_pre= 0.84200 time= 0.01527\n",
      "Epoch: 0200 train_loss= 0.69065 train_acc= 0.88457 val_loss= 0.69146 val_acc= 0.84200 val_pre= 0.84200 time= 0.01490\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.69203 accuracy= 0.78860 precision= 0.78860 recall= 1.00000 time= 0.00755\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "path = 'data/'\n",
    "dataset_str = 'Yelp'\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'Yelp', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed', 'yelp', 'amazone'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# # Load data\n",
    "# adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\"\"\"\n",
    "Loads input data from gcn/data directory\n",
    "\n",
    "ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "    (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "    object;\n",
    "ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "All objects above must be saved using python pickle module.\n",
    "\n",
    ":param dataset_str: Dataset name\n",
    ":return: All data input files loaded (as well the training/test data).\n",
    "\"\"\"\n",
    "# train.py\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy, model.precision, model.recall], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "# utils.py\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "    \n",
    "#utils\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "#utils\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "#utils\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(0))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    # r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features_ch = features.dot(r_mat_inv)\n",
    "    return sparse_to_tuple(features_ch)\n",
    "\n",
    "#utils\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []\n",
    "for i in range(len(names)):\n",
    "    with open(path+\"ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "        if sys.version_info > (3, 0):\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "        else:\n",
    "            objects.append(pkl.load(f))\n",
    "            \n",
    "x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "test_idx_reorder = parse_index_file(path+\"ind.{}.test.index\".format(dataset_str))\n",
    "test_idx_range = np.sort(test_idx_reorder)\n",
    "features = sp.vstack((allx, tx)).tolil()\n",
    "features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "labels = np.vstack((ally, ty))\n",
    "labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "idx_test = test_idx_range.tolist()\n",
    "idx_train = range(len(y))\n",
    "idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "y_train = np.zeros(labels.shape)\n",
    "y_val = np.zeros(labels.shape)\n",
    "y_test = np.zeros(labels.shape)\n",
    "y_train[train_mask, :] = labels[train_mask, :]\n",
    "y_val[val_mask, :] = labels[val_mask, :]\n",
    "y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "\n",
    "# print(features[0:10])\n",
    "# Some preprocessing\n",
    "features_ch = preprocess_features(features)\n",
    "# print(features_ch[2])\n",
    "\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features_ch[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features_ch[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features_ch, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, pre, recall, duration = evaluate(features_ch, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"val_pre=\", \"{:.5f}\".format(pre), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_pre, test_recall, test_duration = evaluate(features_ch, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"precision=\", \"{:.5f}\".format(test_pre),\"recall=\",\"{:.5f}\".format(test_recall),\"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'saeed': 2}}\n"
     ]
    }
   ],
   "source": [
    "temp_dic = {}\n",
    "temp_list = [[1, 2, 3], [4, 5, 5]]\n",
    "temp_dic[1] = {}\n",
    "temp_dic[1]['saeed'] = 2\n",
    "print(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
