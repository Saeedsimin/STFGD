{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uniwa/students6/students/22458906/linux/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/uniwa/students6/students/22458906/linux/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.69464 train_acc= 0.74306 val_loss= 0.69457 val_acc= 0.78600 val_pre= 0.80042 time= 0.09273\n",
      "Epoch: 0002 train_loss= 0.69436 train_acc= 0.75142 val_loss= 0.69426 val_acc= 0.78800 val_pre= 0.79234 time= 0.00807\n",
      "Epoch: 0003 train_loss= 0.69416 train_acc= 0.75409 val_loss= 0.69396 val_acc= 0.78800 val_pre= 0.79234 time= 0.00695\n",
      "Epoch: 0004 train_loss= 0.69375 train_acc= 0.75789 val_loss= 0.69366 val_acc= 0.79000 val_pre= 0.79158 time= 0.00733\n",
      "Epoch: 0005 train_loss= 0.69332 train_acc= 0.76967 val_loss= 0.69337 val_acc= 0.79200 val_pre= 0.79200 time= 0.00700\n",
      "Epoch: 0006 train_loss= 0.69316 train_acc= 0.77157 val_loss= 0.69308 val_acc= 0.79200 val_pre= 0.79200 time= 0.00735\n",
      "Epoch: 0007 train_loss= 0.69293 train_acc= 0.77043 val_loss= 0.69279 val_acc= 0.79200 val_pre= 0.79200 time= 0.00664\n",
      "Epoch: 0008 train_loss= 0.69263 train_acc= 0.77157 val_loss= 0.69250 val_acc= 0.79200 val_pre= 0.79200 time= 0.00706\n",
      "Epoch: 0009 train_loss= 0.69204 train_acc= 0.77271 val_loss= 0.69221 val_acc= 0.79200 val_pre= 0.79200 time= 0.00677\n",
      "Epoch: 0010 train_loss= 0.69190 train_acc= 0.77309 val_loss= 0.69193 val_acc= 0.79200 val_pre= 0.79200 time= 0.00718\n",
      "Epoch: 0011 train_loss= 0.69170 train_acc= 0.77157 val_loss= 0.69164 val_acc= 0.79200 val_pre= 0.79200 time= 0.00701\n",
      "Epoch: 0012 train_loss= 0.69145 train_acc= 0.77347 val_loss= 0.69136 val_acc= 0.79200 val_pre= 0.79200 time= 0.00760\n",
      "Epoch: 0013 train_loss= 0.69130 train_acc= 0.77309 val_loss= 0.69108 val_acc= 0.79200 val_pre= 0.79200 time= 0.00707\n",
      "Epoch: 0014 train_loss= 0.69086 train_acc= 0.77385 val_loss= 0.69080 val_acc= 0.79200 val_pre= 0.79200 time= 0.00773\n",
      "Epoch: 0015 train_loss= 0.69024 train_acc= 0.77385 val_loss= 0.69051 val_acc= 0.79200 val_pre= 0.79200 time= 0.00736\n",
      "Epoch: 0016 train_loss= 0.69008 train_acc= 0.77347 val_loss= 0.69023 val_acc= 0.79200 val_pre= 0.79200 time= 0.00668\n",
      "Epoch: 0017 train_loss= 0.68975 train_acc= 0.77385 val_loss= 0.68994 val_acc= 0.79200 val_pre= 0.79200 time= 0.00727\n",
      "Epoch: 0018 train_loss= 0.68960 train_acc= 0.77385 val_loss= 0.68966 val_acc= 0.79200 val_pre= 0.79200 time= 0.00753\n",
      "Epoch: 0019 train_loss= 0.68917 train_acc= 0.77233 val_loss= 0.68937 val_acc= 0.79200 val_pre= 0.79200 time= 0.00763\n",
      "Epoch: 0020 train_loss= 0.68894 train_acc= 0.77347 val_loss= 0.68907 val_acc= 0.79200 val_pre= 0.79200 time= 0.00732\n",
      "Epoch: 0021 train_loss= 0.68833 train_acc= 0.77385 val_loss= 0.68878 val_acc= 0.79200 val_pre= 0.79200 time= 0.00739\n",
      "Epoch: 0022 train_loss= 0.68854 train_acc= 0.77385 val_loss= 0.68848 val_acc= 0.79200 val_pre= 0.79200 time= 0.00667\n",
      "Epoch: 0023 train_loss= 0.68821 train_acc= 0.77385 val_loss= 0.68817 val_acc= 0.79200 val_pre= 0.79200 time= 0.00699\n",
      "Epoch: 0024 train_loss= 0.68732 train_acc= 0.77385 val_loss= 0.68786 val_acc= 0.79200 val_pre= 0.79200 time= 0.00696\n",
      "Epoch: 0025 train_loss= 0.68725 train_acc= 0.77385 val_loss= 0.68755 val_acc= 0.79200 val_pre= 0.79200 time= 0.00686\n",
      "Epoch: 0026 train_loss= 0.68684 train_acc= 0.77385 val_loss= 0.68723 val_acc= 0.79200 val_pre= 0.79200 time= 0.00714\n",
      "Epoch: 0027 train_loss= 0.68685 train_acc= 0.77385 val_loss= 0.68690 val_acc= 0.79200 val_pre= 0.79200 time= 0.00697\n",
      "Epoch: 0028 train_loss= 0.68580 train_acc= 0.77385 val_loss= 0.68657 val_acc= 0.79200 val_pre= 0.79200 time= 0.00714\n",
      "Epoch: 0029 train_loss= 0.68605 train_acc= 0.77385 val_loss= 0.68624 val_acc= 0.79200 val_pre= 0.79200 time= 0.00856\n",
      "Epoch: 0030 train_loss= 0.68535 train_acc= 0.77385 val_loss= 0.68589 val_acc= 0.79200 val_pre= 0.79200 time= 0.00812\n",
      "Epoch: 0031 train_loss= 0.68537 train_acc= 0.77385 val_loss= 0.68554 val_acc= 0.79200 val_pre= 0.79200 time= 0.00724\n",
      "Epoch: 0032 train_loss= 0.68532 train_acc= 0.77385 val_loss= 0.68519 val_acc= 0.79200 val_pre= 0.79200 time= 0.00706\n",
      "Epoch: 0033 train_loss= 0.68354 train_acc= 0.77385 val_loss= 0.68482 val_acc= 0.79200 val_pre= 0.79200 time= 0.00666\n",
      "Epoch: 0034 train_loss= 0.68357 train_acc= 0.77385 val_loss= 0.68445 val_acc= 0.79200 val_pre= 0.79200 time= 0.00693\n",
      "Epoch: 0035 train_loss= 0.68329 train_acc= 0.77385 val_loss= 0.68406 val_acc= 0.79200 val_pre= 0.79200 time= 0.00739\n",
      "Epoch: 0036 train_loss= 0.68237 train_acc= 0.77385 val_loss= 0.68367 val_acc= 0.79200 val_pre= 0.79200 time= 0.00707\n",
      "Epoch: 0037 train_loss= 0.68292 train_acc= 0.77385 val_loss= 0.68327 val_acc= 0.79200 val_pre= 0.79200 time= 0.00663\n",
      "Epoch: 0038 train_loss= 0.68183 train_acc= 0.77385 val_loss= 0.68286 val_acc= 0.79200 val_pre= 0.79200 time= 0.00973\n",
      "Epoch: 0039 train_loss= 0.68095 train_acc= 0.77385 val_loss= 0.68245 val_acc= 0.79200 val_pre= 0.79200 time= 0.00728\n",
      "Epoch: 0040 train_loss= 0.68197 train_acc= 0.77385 val_loss= 0.68203 val_acc= 0.79200 val_pre= 0.79200 time= 0.00707\n",
      "Epoch: 0041 train_loss= 0.68111 train_acc= 0.77385 val_loss= 0.68159 val_acc= 0.79200 val_pre= 0.79200 time= 0.00690\n",
      "Epoch: 0042 train_loss= 0.68073 train_acc= 0.77385 val_loss= 0.68115 val_acc= 0.79200 val_pre= 0.79200 time= 0.00654\n",
      "Epoch: 0043 train_loss= 0.68010 train_acc= 0.77385 val_loss= 0.68070 val_acc= 0.79200 val_pre= 0.79200 time= 0.00679\n",
      "Epoch: 0044 train_loss= 0.68034 train_acc= 0.77385 val_loss= 0.68025 val_acc= 0.79200 val_pre= 0.79200 time= 0.00721\n",
      "Epoch: 0045 train_loss= 0.67902 train_acc= 0.77385 val_loss= 0.67979 val_acc= 0.79200 val_pre= 0.79200 time= 0.00700\n",
      "Epoch: 0046 train_loss= 0.68016 train_acc= 0.77385 val_loss= 0.67932 val_acc= 0.79200 val_pre= 0.79200 time= 0.00724\n",
      "Epoch: 0047 train_loss= 0.67858 train_acc= 0.77385 val_loss= 0.67884 val_acc= 0.79200 val_pre= 0.79200 time= 0.00715\n",
      "Epoch: 0048 train_loss= 0.67791 train_acc= 0.77385 val_loss= 0.67835 val_acc= 0.79200 val_pre= 0.79200 time= 0.00723\n",
      "Epoch: 0049 train_loss= 0.67790 train_acc= 0.77385 val_loss= 0.67786 val_acc= 0.79200 val_pre= 0.79200 time= 0.00689\n",
      "Epoch: 0050 train_loss= 0.67472 train_acc= 0.77385 val_loss= 0.67735 val_acc= 0.79200 val_pre= 0.79200 time= 0.00733\n",
      "Epoch: 0051 train_loss= 0.67741 train_acc= 0.77385 val_loss= 0.67684 val_acc= 0.79200 val_pre= 0.79200 time= 0.00709\n",
      "Epoch: 0052 train_loss= 0.67523 train_acc= 0.77385 val_loss= 0.67631 val_acc= 0.79200 val_pre= 0.79200 time= 0.00731\n",
      "Epoch: 0053 train_loss= 0.67384 train_acc= 0.77385 val_loss= 0.67578 val_acc= 0.79200 val_pre= 0.79200 time= 0.00716\n",
      "Epoch: 0054 train_loss= 0.67432 train_acc= 0.77385 val_loss= 0.67523 val_acc= 0.79200 val_pre= 0.79200 time= 0.00712\n",
      "Epoch: 0055 train_loss= 0.67373 train_acc= 0.77385 val_loss= 0.67468 val_acc= 0.79200 val_pre= 0.79200 time= 0.00770\n",
      "Epoch: 0056 train_loss= 0.67245 train_acc= 0.77385 val_loss= 0.67412 val_acc= 0.79200 val_pre= 0.79200 time= 0.00772\n",
      "Epoch: 0057 train_loss= 0.67372 train_acc= 0.77385 val_loss= 0.67354 val_acc= 0.79200 val_pre= 0.79200 time= 0.00856\n",
      "Epoch: 0058 train_loss= 0.67149 train_acc= 0.77385 val_loss= 0.67296 val_acc= 0.79200 val_pre= 0.79200 time= 0.00721\n",
      "Epoch: 0059 train_loss= 0.67331 train_acc= 0.77385 val_loss= 0.67238 val_acc= 0.79200 val_pre= 0.79200 time= 0.00690\n",
      "Epoch: 0060 train_loss= 0.66995 train_acc= 0.77385 val_loss= 0.67178 val_acc= 0.79200 val_pre= 0.79200 time= 0.00700\n",
      "Epoch: 0061 train_loss= 0.67184 train_acc= 0.77385 val_loss= 0.67118 val_acc= 0.79200 val_pre= 0.79200 time= 0.00683\n",
      "Epoch: 0062 train_loss= 0.66815 train_acc= 0.77385 val_loss= 0.67056 val_acc= 0.79200 val_pre= 0.79200 time= 0.00663\n",
      "Epoch: 0063 train_loss= 0.67038 train_acc= 0.77385 val_loss= 0.66994 val_acc= 0.79200 val_pre= 0.79200 time= 0.00718\n",
      "Epoch: 0064 train_loss= 0.66680 train_acc= 0.77385 val_loss= 0.66930 val_acc= 0.79200 val_pre= 0.79200 time= 0.00708\n",
      "Epoch: 0065 train_loss= 0.66710 train_acc= 0.77385 val_loss= 0.66866 val_acc= 0.79200 val_pre= 0.79200 time= 0.00740\n",
      "Epoch: 0066 train_loss= 0.66646 train_acc= 0.77385 val_loss= 0.66802 val_acc= 0.79200 val_pre= 0.79200 time= 0.00831\n",
      "Epoch: 0067 train_loss= 0.66913 train_acc= 0.77385 val_loss= 0.66737 val_acc= 0.79200 val_pre= 0.79200 time= 0.00691\n",
      "Epoch: 0068 train_loss= 0.66697 train_acc= 0.77385 val_loss= 0.66672 val_acc= 0.79200 val_pre= 0.79200 time= 0.00692\n",
      "Epoch: 0069 train_loss= 0.66631 train_acc= 0.77385 val_loss= 0.66606 val_acc= 0.79200 val_pre= 0.79200 time= 0.00699\n",
      "Epoch: 0070 train_loss= 0.66622 train_acc= 0.77385 val_loss= 0.66540 val_acc= 0.79200 val_pre= 0.79200 time= 0.00667\n",
      "Epoch: 0071 train_loss= 0.66556 train_acc= 0.77385 val_loss= 0.66474 val_acc= 0.79200 val_pre= 0.79200 time= 0.00721\n",
      "Epoch: 0072 train_loss= 0.66345 train_acc= 0.77385 val_loss= 0.66407 val_acc= 0.79200 val_pre= 0.79200 time= 0.00717\n",
      "Epoch: 0073 train_loss= 0.66281 train_acc= 0.77385 val_loss= 0.66339 val_acc= 0.79200 val_pre= 0.79200 time= 0.00711\n",
      "Epoch: 0074 train_loss= 0.66405 train_acc= 0.77385 val_loss= 0.66271 val_acc= 0.79200 val_pre= 0.79200 time= 0.00669\n",
      "Epoch: 0075 train_loss= 0.66275 train_acc= 0.77385 val_loss= 0.66203 val_acc= 0.79200 val_pre= 0.79200 time= 0.00719\n",
      "Epoch: 0076 train_loss= 0.65529 train_acc= 0.77385 val_loss= 0.66133 val_acc= 0.79200 val_pre= 0.79200 time= 0.00740\n",
      "Epoch: 0077 train_loss= 0.65969 train_acc= 0.77385 val_loss= 0.66062 val_acc= 0.79200 val_pre= 0.79200 time= 0.00717\n",
      "Epoch: 0078 train_loss= 0.65933 train_acc= 0.77385 val_loss= 0.65991 val_acc= 0.79200 val_pre= 0.79200 time= 0.00666\n",
      "Epoch: 0079 train_loss= 0.66062 train_acc= 0.77385 val_loss= 0.65921 val_acc= 0.79200 val_pre= 0.79200 time= 0.00726\n",
      "Epoch: 0080 train_loss= 0.66017 train_acc= 0.77385 val_loss= 0.65850 val_acc= 0.79200 val_pre= 0.79200 time= 0.00697\n",
      "Epoch: 0081 train_loss= 0.65643 train_acc= 0.77385 val_loss= 0.65779 val_acc= 0.79200 val_pre= 0.79200 time= 0.00686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0082 train_loss= 0.65872 train_acc= 0.77385 val_loss= 0.65708 val_acc= 0.79200 val_pre= 0.79200 time= 0.00727\n",
      "Epoch: 0083 train_loss= 0.65700 train_acc= 0.77385 val_loss= 0.65637 val_acc= 0.79200 val_pre= 0.79200 time= 0.00748\n",
      "Epoch: 0084 train_loss= 0.65866 train_acc= 0.77385 val_loss= 0.65567 val_acc= 0.79200 val_pre= 0.79200 time= 0.00756\n",
      "Epoch: 0085 train_loss= 0.65210 train_acc= 0.77385 val_loss= 0.65496 val_acc= 0.79200 val_pre= 0.79200 time= 0.00744\n",
      "Epoch: 0086 train_loss= 0.65660 train_acc= 0.77385 val_loss= 0.65426 val_acc= 0.79200 val_pre= 0.79200 time= 0.00696\n",
      "Epoch: 0087 train_loss= 0.65285 train_acc= 0.77385 val_loss= 0.65355 val_acc= 0.79200 val_pre= 0.79200 time= 0.00702\n",
      "Epoch: 0088 train_loss= 0.65192 train_acc= 0.77385 val_loss= 0.65284 val_acc= 0.79200 val_pre= 0.79200 time= 0.00710\n",
      "Epoch: 0089 train_loss= 0.65054 train_acc= 0.77385 val_loss= 0.65214 val_acc= 0.79200 val_pre= 0.79200 time= 0.00727\n",
      "Epoch: 0090 train_loss= 0.65172 train_acc= 0.77385 val_loss= 0.65144 val_acc= 0.79200 val_pre= 0.79200 time= 0.00715\n",
      "Epoch: 0091 train_loss= 0.65204 train_acc= 0.77385 val_loss= 0.65074 val_acc= 0.79200 val_pre= 0.79200 time= 0.00831\n",
      "Epoch: 0092 train_loss= 0.64996 train_acc= 0.77385 val_loss= 0.65003 val_acc= 0.79200 val_pre= 0.79200 time= 0.00723\n",
      "Epoch: 0093 train_loss= 0.64848 train_acc= 0.77385 val_loss= 0.64933 val_acc= 0.79200 val_pre= 0.79200 time= 0.00702\n",
      "Epoch: 0094 train_loss= 0.64768 train_acc= 0.77385 val_loss= 0.64863 val_acc= 0.79200 val_pre= 0.79200 time= 0.00699\n",
      "Epoch: 0095 train_loss= 0.64811 train_acc= 0.77385 val_loss= 0.64793 val_acc= 0.79200 val_pre= 0.79200 time= 0.00698\n",
      "Epoch: 0096 train_loss= 0.65227 train_acc= 0.77385 val_loss= 0.64724 val_acc= 0.79200 val_pre= 0.79200 time= 0.00693\n",
      "Epoch: 0097 train_loss= 0.65071 train_acc= 0.77385 val_loss= 0.64656 val_acc= 0.79200 val_pre= 0.79200 time= 0.00702\n",
      "Epoch: 0098 train_loss= 0.64574 train_acc= 0.77385 val_loss= 0.64588 val_acc= 0.79200 val_pre= 0.79200 time= 0.00656\n",
      "Epoch: 0099 train_loss= 0.64933 train_acc= 0.77385 val_loss= 0.64520 val_acc= 0.79200 val_pre= 0.79200 time= 0.00737\n",
      "Epoch: 0100 train_loss= 0.65077 train_acc= 0.77385 val_loss= 0.64454 val_acc= 0.79200 val_pre= 0.79200 time= 0.00701\n",
      "Epoch: 0101 train_loss= 0.64604 train_acc= 0.77385 val_loss= 0.64388 val_acc= 0.79200 val_pre= 0.79200 time= 0.00692\n",
      "Epoch: 0102 train_loss= 0.64295 train_acc= 0.77385 val_loss= 0.64321 val_acc= 0.79200 val_pre= 0.79200 time= 0.00747\n",
      "Epoch: 0103 train_loss= 0.64332 train_acc= 0.77385 val_loss= 0.64256 val_acc= 0.79200 val_pre= 0.79200 time= 0.00725\n",
      "Epoch: 0104 train_loss= 0.64730 train_acc= 0.77385 val_loss= 0.64191 val_acc= 0.79200 val_pre= 0.79200 time= 0.00685\n",
      "Epoch: 0105 train_loss= 0.64109 train_acc= 0.77385 val_loss= 0.64126 val_acc= 0.79200 val_pre= 0.79200 time= 0.00712\n",
      "Epoch: 0106 train_loss= 0.64281 train_acc= 0.77385 val_loss= 0.64062 val_acc= 0.79200 val_pre= 0.79200 time= 0.00671\n",
      "Epoch: 0107 train_loss= 0.63828 train_acc= 0.77385 val_loss= 0.63998 val_acc= 0.79200 val_pre= 0.79200 time= 0.00676\n",
      "Epoch: 0108 train_loss= 0.63946 train_acc= 0.77385 val_loss= 0.63933 val_acc= 0.79200 val_pre= 0.79200 time= 0.00694\n",
      "Epoch: 0109 train_loss= 0.64385 train_acc= 0.77385 val_loss= 0.63870 val_acc= 0.79200 val_pre= 0.79200 time= 0.00699\n",
      "Epoch: 0110 train_loss= 0.64240 train_acc= 0.77385 val_loss= 0.63808 val_acc= 0.79200 val_pre= 0.79200 time= 0.00769\n",
      "Epoch: 0111 train_loss= 0.63983 train_acc= 0.77385 val_loss= 0.63746 val_acc= 0.79200 val_pre= 0.79200 time= 0.00744\n",
      "Epoch: 0112 train_loss= 0.63454 train_acc= 0.77385 val_loss= 0.63683 val_acc= 0.79200 val_pre= 0.79200 time= 0.00731\n",
      "Epoch: 0113 train_loss= 0.63675 train_acc= 0.77385 val_loss= 0.63622 val_acc= 0.79200 val_pre= 0.79200 time= 0.00731\n",
      "Epoch: 0114 train_loss= 0.64113 train_acc= 0.77385 val_loss= 0.63561 val_acc= 0.79200 val_pre= 0.79200 time= 0.00702\n",
      "Epoch: 0115 train_loss= 0.64036 train_acc= 0.77385 val_loss= 0.63501 val_acc= 0.79200 val_pre= 0.79200 time= 0.00716\n",
      "Epoch: 0116 train_loss= 0.63832 train_acc= 0.77385 val_loss= 0.63442 val_acc= 0.79200 val_pre= 0.79200 time= 0.00699\n",
      "Epoch: 0117 train_loss= 0.63525 train_acc= 0.77385 val_loss= 0.63384 val_acc= 0.79200 val_pre= 0.79200 time= 0.00678\n",
      "Epoch: 0118 train_loss= 0.63383 train_acc= 0.77385 val_loss= 0.63326 val_acc= 0.79200 val_pre= 0.79200 time= 0.00841\n",
      "Epoch: 0119 train_loss= 0.63294 train_acc= 0.77385 val_loss= 0.63268 val_acc= 0.79200 val_pre= 0.79200 time= 0.00724\n",
      "Epoch: 0120 train_loss= 0.63781 train_acc= 0.77385 val_loss= 0.63212 val_acc= 0.79200 val_pre= 0.79200 time= 0.00735\n",
      "Epoch: 0121 train_loss= 0.63289 train_acc= 0.77385 val_loss= 0.63156 val_acc= 0.79200 val_pre= 0.79200 time= 0.00680\n",
      "Epoch: 0122 train_loss= 0.63488 train_acc= 0.77385 val_loss= 0.63101 val_acc= 0.79200 val_pre= 0.79200 time= 0.00739\n",
      "Epoch: 0123 train_loss= 0.63451 train_acc= 0.77385 val_loss= 0.63046 val_acc= 0.79200 val_pre= 0.79200 time= 0.00675\n",
      "Epoch: 0124 train_loss= 0.63375 train_acc= 0.77385 val_loss= 0.62992 val_acc= 0.79200 val_pre= 0.79200 time= 0.00696\n",
      "Epoch: 0125 train_loss= 0.63533 train_acc= 0.77385 val_loss= 0.62940 val_acc= 0.79200 val_pre= 0.79200 time= 0.00703\n",
      "Epoch: 0126 train_loss= 0.62918 train_acc= 0.77385 val_loss= 0.62888 val_acc= 0.79200 val_pre= 0.79200 time= 0.00684\n",
      "Epoch: 0127 train_loss= 0.63638 train_acc= 0.77385 val_loss= 0.62837 val_acc= 0.79200 val_pre= 0.79200 time= 0.00730\n",
      "Epoch: 0128 train_loss= 0.63141 train_acc= 0.77385 val_loss= 0.62786 val_acc= 0.79200 val_pre= 0.79200 time= 0.00703\n",
      "Epoch: 0129 train_loss= 0.62730 train_acc= 0.77385 val_loss= 0.62735 val_acc= 0.79200 val_pre= 0.79200 time= 0.00693\n",
      "Epoch: 0130 train_loss= 0.63578 train_acc= 0.77385 val_loss= 0.62686 val_acc= 0.79200 val_pre= 0.79200 time= 0.00694\n",
      "Epoch: 0131 train_loss= 0.62887 train_acc= 0.77385 val_loss= 0.62637 val_acc= 0.79200 val_pre= 0.79200 time= 0.00714\n",
      "Epoch: 0132 train_loss= 0.63151 train_acc= 0.77385 val_loss= 0.62590 val_acc= 0.79200 val_pre= 0.79200 time= 0.00645\n",
      "Epoch: 0133 train_loss= 0.62898 train_acc= 0.77385 val_loss= 0.62543 val_acc= 0.79200 val_pre= 0.79200 time= 0.00679\n",
      "Epoch: 0134 train_loss= 0.62948 train_acc= 0.77385 val_loss= 0.62496 val_acc= 0.79200 val_pre= 0.79200 time= 0.00717\n",
      "Epoch: 0135 train_loss= 0.63314 train_acc= 0.77385 val_loss= 0.62451 val_acc= 0.79200 val_pre= 0.79200 time= 0.00728\n",
      "Epoch: 0136 train_loss= 0.63372 train_acc= 0.77385 val_loss= 0.62407 val_acc= 0.79200 val_pre= 0.79200 time= 0.00738\n",
      "Epoch: 0137 train_loss= 0.63540 train_acc= 0.77385 val_loss= 0.62365 val_acc= 0.79200 val_pre= 0.79200 time= 0.00806\n",
      "Epoch: 0138 train_loss= 0.63082 train_acc= 0.77385 val_loss= 0.62322 val_acc= 0.79200 val_pre= 0.79200 time= 0.00729\n",
      "Epoch: 0139 train_loss= 0.63214 train_acc= 0.77385 val_loss= 0.62281 val_acc= 0.79200 val_pre= 0.79200 time= 0.00745\n",
      "Epoch: 0140 train_loss= 0.62763 train_acc= 0.77385 val_loss= 0.62240 val_acc= 0.79200 val_pre= 0.79200 time= 0.00705\n",
      "Epoch: 0141 train_loss= 0.62810 train_acc= 0.77385 val_loss= 0.62201 val_acc= 0.79200 val_pre= 0.79200 time= 0.00689\n",
      "Epoch: 0142 train_loss= 0.63040 train_acc= 0.77385 val_loss= 0.62162 val_acc= 0.79200 val_pre= 0.79200 time= 0.00722\n",
      "Epoch: 0143 train_loss= 0.63330 train_acc= 0.77385 val_loss= 0.62125 val_acc= 0.79200 val_pre= 0.79200 time= 0.00765\n",
      "Epoch: 0144 train_loss= 0.63305 train_acc= 0.77385 val_loss= 0.62089 val_acc= 0.79200 val_pre= 0.79200 time= 0.00736\n",
      "Epoch: 0145 train_loss= 0.62861 train_acc= 0.77385 val_loss= 0.62054 val_acc= 0.79200 val_pre= 0.79200 time= 0.00760\n",
      "Epoch: 0146 train_loss= 0.62788 train_acc= 0.77385 val_loss= 0.62020 val_acc= 0.79200 val_pre= 0.79200 time= 0.00819\n",
      "Epoch: 0147 train_loss= 0.62867 train_acc= 0.77385 val_loss= 0.61986 val_acc= 0.79200 val_pre= 0.79200 time= 0.00721\n",
      "Epoch: 0148 train_loss= 0.62754 train_acc= 0.77385 val_loss= 0.61951 val_acc= 0.79200 val_pre= 0.79200 time= 0.00680\n",
      "Epoch: 0149 train_loss= 0.62619 train_acc= 0.77385 val_loss= 0.61919 val_acc= 0.79200 val_pre= 0.79200 time= 0.00716\n",
      "Epoch: 0150 train_loss= 0.63187 train_acc= 0.77385 val_loss= 0.61888 val_acc= 0.79200 val_pre= 0.79200 time= 0.00726\n",
      "Epoch: 0151 train_loss= 0.63005 train_acc= 0.77385 val_loss= 0.61858 val_acc= 0.79200 val_pre= 0.79200 time= 0.00703\n",
      "Epoch: 0152 train_loss= 0.62354 train_acc= 0.77385 val_loss= 0.61829 val_acc= 0.79200 val_pre= 0.79200 time= 0.00724\n",
      "Epoch: 0153 train_loss= 0.62893 train_acc= 0.77385 val_loss= 0.61800 val_acc= 0.79200 val_pre= 0.79200 time= 0.00705\n",
      "Epoch: 0154 train_loss= 0.62205 train_acc= 0.77385 val_loss= 0.61771 val_acc= 0.79200 val_pre= 0.79200 time= 0.00678\n",
      "Epoch: 0155 train_loss= 0.62660 train_acc= 0.77385 val_loss= 0.61743 val_acc= 0.79200 val_pre= 0.79200 time= 0.00722\n",
      "Epoch: 0156 train_loss= 0.62700 train_acc= 0.77385 val_loss= 0.61716 val_acc= 0.79200 val_pre= 0.79200 time= 0.00736\n",
      "Epoch: 0157 train_loss= 0.63347 train_acc= 0.77385 val_loss= 0.61690 val_acc= 0.79200 val_pre= 0.79200 time= 0.00706\n",
      "Epoch: 0158 train_loss= 0.62463 train_acc= 0.77385 val_loss= 0.61665 val_acc= 0.79200 val_pre= 0.79200 time= 0.00732\n",
      "Epoch: 0159 train_loss= 0.62483 train_acc= 0.77385 val_loss= 0.61640 val_acc= 0.79200 val_pre= 0.79200 time= 0.00698\n",
      "Epoch: 0160 train_loss= 0.62799 train_acc= 0.77385 val_loss= 0.61616 val_acc= 0.79200 val_pre= 0.79200 time= 0.00704\n",
      "Epoch: 0161 train_loss= 0.62599 train_acc= 0.77385 val_loss= 0.61592 val_acc= 0.79200 val_pre= 0.79200 time= 0.00753\n",
      "Epoch: 0162 train_loss= 0.62748 train_acc= 0.77385 val_loss= 0.61568 val_acc= 0.79200 val_pre= 0.79200 time= 0.00686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0163 train_loss= 0.61844 train_acc= 0.77385 val_loss= 0.61543 val_acc= 0.79200 val_pre= 0.79200 time= 0.00741\n",
      "Epoch: 0164 train_loss= 0.62640 train_acc= 0.77385 val_loss= 0.61519 val_acc= 0.79200 val_pre= 0.79200 time= 0.00770\n",
      "Epoch: 0165 train_loss= 0.62594 train_acc= 0.77385 val_loss= 0.61495 val_acc= 0.79200 val_pre= 0.79200 time= 0.00721\n",
      "Epoch: 0166 train_loss= 0.61624 train_acc= 0.77385 val_loss= 0.61470 val_acc= 0.79200 val_pre= 0.79200 time= 0.00750\n",
      "Epoch: 0167 train_loss= 0.62536 train_acc= 0.77385 val_loss= 0.61447 val_acc= 0.79200 val_pre= 0.79200 time= 0.00704\n",
      "Epoch: 0168 train_loss= 0.61920 train_acc= 0.77385 val_loss= 0.61424 val_acc= 0.79200 val_pre= 0.79200 time= 0.00705\n",
      "Epoch: 0169 train_loss= 0.62798 train_acc= 0.77385 val_loss= 0.61401 val_acc= 0.79200 val_pre= 0.79200 time= 0.00693\n",
      "Epoch: 0170 train_loss= 0.62195 train_acc= 0.77385 val_loss= 0.61379 val_acc= 0.79200 val_pre= 0.79200 time= 0.00720\n",
      "Epoch: 0171 train_loss= 0.62075 train_acc= 0.77385 val_loss= 0.61356 val_acc= 0.79200 val_pre= 0.79200 time= 0.00678\n",
      "Epoch: 0172 train_loss= 0.62517 train_acc= 0.77385 val_loss= 0.61335 val_acc= 0.79200 val_pre= 0.79200 time= 0.00935\n",
      "Epoch: 0173 train_loss= 0.61829 train_acc= 0.77385 val_loss= 0.61313 val_acc= 0.79200 val_pre= 0.79200 time= 0.00718\n",
      "Epoch: 0174 train_loss= 0.62429 train_acc= 0.77385 val_loss= 0.61291 val_acc= 0.79200 val_pre= 0.79200 time= 0.00692\n",
      "Epoch: 0175 train_loss= 0.62912 train_acc= 0.77385 val_loss= 0.61271 val_acc= 0.79200 val_pre= 0.79200 time= 0.00742\n",
      "Epoch: 0176 train_loss= 0.62412 train_acc= 0.77385 val_loss= 0.61252 val_acc= 0.79200 val_pre= 0.79200 time= 0.00670\n",
      "Epoch: 0177 train_loss= 0.62425 train_acc= 0.77385 val_loss= 0.61233 val_acc= 0.79200 val_pre= 0.79200 time= 0.00718\n",
      "Epoch: 0178 train_loss= 0.62142 train_acc= 0.77385 val_loss= 0.61214 val_acc= 0.79200 val_pre= 0.79200 time= 0.00710\n",
      "Epoch: 0179 train_loss= 0.61533 train_acc= 0.77385 val_loss= 0.61195 val_acc= 0.79200 val_pre= 0.79200 time= 0.00678\n",
      "Epoch: 0180 train_loss= 0.62145 train_acc= 0.77385 val_loss= 0.61175 val_acc= 0.79200 val_pre= 0.79200 time= 0.00745\n",
      "Epoch: 0181 train_loss= 0.62414 train_acc= 0.77385 val_loss= 0.61157 val_acc= 0.79200 val_pre= 0.79200 time= 0.00671\n",
      "Epoch: 0182 train_loss= 0.62374 train_acc= 0.77385 val_loss= 0.61139 val_acc= 0.79200 val_pre= 0.79200 time= 0.00675\n",
      "Epoch: 0183 train_loss= 0.62187 train_acc= 0.77385 val_loss= 0.61122 val_acc= 0.79200 val_pre= 0.79200 time= 0.00705\n",
      "Epoch: 0184 train_loss= 0.62102 train_acc= 0.77385 val_loss= 0.61105 val_acc= 0.79200 val_pre= 0.79200 time= 0.00654\n",
      "Epoch: 0185 train_loss= 0.61703 train_acc= 0.77385 val_loss= 0.61087 val_acc= 0.79200 val_pre= 0.79200 time= 0.00678\n",
      "Epoch: 0186 train_loss= 0.61313 train_acc= 0.77385 val_loss= 0.61069 val_acc= 0.79200 val_pre= 0.79200 time= 0.00699\n",
      "Epoch: 0187 train_loss= 0.62338 train_acc= 0.77385 val_loss= 0.61051 val_acc= 0.79200 val_pre= 0.79200 time= 0.00710\n",
      "Epoch: 0188 train_loss= 0.62321 train_acc= 0.77385 val_loss= 0.61034 val_acc= 0.79200 val_pre= 0.79200 time= 0.00706\n",
      "Epoch: 0189 train_loss= 0.62327 train_acc= 0.77385 val_loss= 0.61017 val_acc= 0.79200 val_pre= 0.79200 time= 0.00696\n",
      "Epoch: 0190 train_loss= 0.61532 train_acc= 0.77385 val_loss= 0.61000 val_acc= 0.79200 val_pre= 0.79200 time= 0.00731\n",
      "Epoch: 0191 train_loss= 0.61709 train_acc= 0.77385 val_loss= 0.60984 val_acc= 0.79200 val_pre= 0.79200 time= 0.00738\n",
      "Epoch: 0192 train_loss= 0.62160 train_acc= 0.77385 val_loss= 0.60968 val_acc= 0.79200 val_pre= 0.79200 time= 0.00665\n",
      "Epoch: 0193 train_loss= 0.61586 train_acc= 0.77385 val_loss= 0.60952 val_acc= 0.79200 val_pre= 0.79200 time= 0.00760\n",
      "Epoch: 0194 train_loss= 0.62353 train_acc= 0.77385 val_loss= 0.60937 val_acc= 0.79200 val_pre= 0.79200 time= 0.00712\n",
      "Epoch: 0195 train_loss= 0.62517 train_acc= 0.77385 val_loss= 0.60921 val_acc= 0.79200 val_pre= 0.79200 time= 0.00716\n",
      "Epoch: 0196 train_loss= 0.62132 train_acc= 0.77385 val_loss= 0.60907 val_acc= 0.79200 val_pre= 0.79200 time= 0.00757\n",
      "Epoch: 0197 train_loss= 0.61746 train_acc= 0.77385 val_loss= 0.60893 val_acc= 0.79200 val_pre= 0.79200 time= 0.00677\n",
      "Epoch: 0198 train_loss= 0.62300 train_acc= 0.77385 val_loss= 0.60880 val_acc= 0.79200 val_pre= 0.79200 time= 0.00721\n",
      "Epoch: 0199 train_loss= 0.62695 train_acc= 0.77385 val_loss= 0.60870 val_acc= 0.79200 val_pre= 0.79200 time= 0.00718\n",
      "Epoch: 0200 train_loss= 0.61846 train_acc= 0.77385 val_loss= 0.60860 val_acc= 0.79200 val_pre= 0.79200 time= 0.00820\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.60160 accuracy= 0.81032 precision= 0.81032 recall= 1.00000 time= 0.00388\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "path = 'data/'\n",
    "dataset_str = 'Amazon'\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'Yelp', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed', 'yelp', 'amazone'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# # Load data\n",
    "# adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\"\"\"\n",
    "Loads input data from gcn/data directory\n",
    "\n",
    "ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "    (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "    object;\n",
    "ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "All objects above must be saved using python pickle module.\n",
    "\n",
    ":param dataset_str: Dataset name\n",
    ":return: All data input files loaded (as well the training/test data).\n",
    "\"\"\"\n",
    "# train.py\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy, model.precision, model.recall], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "# utils.py\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "    \n",
    "#utils\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "#utils\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "#utils\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(0))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    # r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features_ch = features.dot(r_mat_inv)\n",
    "    return sparse_to_tuple(features_ch)\n",
    "\n",
    "#utils\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []\n",
    "for i in range(len(names)):\n",
    "    with open(path+\"ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "        if sys.version_info > (3, 0):\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "        else:\n",
    "            objects.append(pkl.load(f))\n",
    "            \n",
    "x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "test_idx_reorder = parse_index_file(path+\"ind.{}.test.index\".format(dataset_str))\n",
    "test_idx_range = np.sort(test_idx_reorder)\n",
    "features = sp.vstack((allx, tx)).tolil()\n",
    "features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "labels = np.vstack((ally, ty))\n",
    "labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "idx_test = test_idx_range.tolist()\n",
    "idx_train = range(len(y))\n",
    "idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "y_train = np.zeros(labels.shape)\n",
    "y_val = np.zeros(labels.shape)\n",
    "y_test = np.zeros(labels.shape)\n",
    "y_train[train_mask, :] = labels[train_mask, :]\n",
    "y_val[val_mask, :] = labels[val_mask, :]\n",
    "y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "\n",
    "# print(features[0:10])\n",
    "# Some preprocessing\n",
    "features_ch = preprocess_features(features)\n",
    "# print(features_ch[2])\n",
    "\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features_ch[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features_ch[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features_ch, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, pre, recall, duration = evaluate(features_ch, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"val_pre=\", \"{:.5f}\".format(pre), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_pre, test_recall, test_duration = evaluate(features_ch, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"precision=\", \"{:.5f}\".format(test_pre),\"recall=\",\"{:.5f}\".format(test_recall),\"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'saeed': 2}}\n"
     ]
    }
   ],
   "source": [
    "temp_dic = {}\n",
    "temp_list = [[1, 2, 3], [4, 5, 5]]\n",
    "temp_dic[1] = {}\n",
    "temp_dic[1]['saeed'] = 2\n",
    "print(temp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
