{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the GRN \n",
    "from train import *\n",
    "from utils import *\n",
    "from data import *\n",
    "\n",
    "args = Args()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda)\n",
    "print('CUDA', args.cuda)\n",
    "\n",
    "def test_graph(epoch, args, rnn, output, graphs_test, test_batch_size):\n",
    "    number_of_subgraphs = len(graphs_test)\n",
    "    print(number_of_subgraphs)\n",
    "\n",
    "    iter = 0\n",
    "    G_pred_final = []\n",
    "    while iter<number_of_subgraphs:\n",
    "\n",
    "        rnn.hidden = rnn.init_hidden(args.batch_size)\n",
    "        rnn.eval()\n",
    "        output.eval()\n",
    "\n",
    "        num_of_nodes = len(graphs_test[iter].nodes())\n",
    "        atts = nx.get_node_attributes(graphs_test[iter],'feature')\n",
    "        atts_array = [value for _,value in atts.items() ]; \n",
    "\n",
    "        y_pred_long = Variable(torch.zeros(args.test_batch_size, args.max_num_node, args.max_prev_node+len(atts_array[0]))).cuda() # discrete prediction\n",
    "\n",
    "        x_step = np.zeros((args.test_batch_size,1,args.max_prev_node+len(atts_array[0])))\n",
    "        x_step[:,:,0] = 1\n",
    "        x_step[:,:,args.max_prev_node:] = atts_array[0]\n",
    "        # x_step = Variable(torch.ones(args.test_single_batch_size,1,)).cuda()\n",
    "        x_step = torch.from_numpy(x_step).float()\n",
    "        x_step = Variable(x_step).cuda()\n",
    "        for i in range(num_of_nodes):\n",
    "            h = rnn(x_step)\n",
    "            # output.hidden = h.permute(1,0,2)\n",
    "            hidden_null = Variable(torch.zeros(args.num_layers - 1, h.size(0), h.size(2))).cuda()\n",
    "            output.hidden = torch.cat((h.permute(1,0,2), hidden_null),\n",
    "                                      dim=0)  # num_layers, batch_size, hidden_size\n",
    "            x_step = Variable(torch.zeros(args.test_batch_size,1,args.max_prev_node+len(atts_array[0]))).cuda()\n",
    "            output_x_step = Variable(torch.ones(args.test_batch_size,1,1)).cuda()\n",
    "            for j in range(min(args.max_prev_node+len(atts_array[0]),i+1)):\n",
    "                output_y_pred_step = output(output_x_step)\n",
    "                output_x_step = sample_sigmoid(output_y_pred_step, sample=True, sample_time=1)\n",
    "                x_step[:,:,j:j+1] = output_x_step\n",
    "                output.hidden = Variable(output.hidden.data).cuda()\n",
    "            y_pred_long[:, i:i + 1, :] = x_step\n",
    "            rnn.hidden = Variable(rnn.hidden.data).cuda()\n",
    "        y_pred_long_data = y_pred_long.data.long()\n",
    "\n",
    "        # save graphs as pickle\n",
    "        G_pred_list = []\n",
    "        for i in range(args.test_single_batch_size):\n",
    "            adj_pred = decode_adj(y_pred_long_data[i].cpu().numpy())\n",
    "            G_pred = get_graph(adj_pred) # get a graph from zero-padded adj\n",
    "            G_pred_list.append(G_pred)\n",
    "        G_pred_final.extend(G_pred_list)\n",
    "        iter += 1\n",
    "    print(len(G_pred_final))\n",
    "    # save graphs\n",
    "    fname = args.graph_save_path + args.fname_pred + str(epoch) + '.dat'\n",
    "    print(fname)\n",
    "    save_graph_list(G_pred_final, fname)\n",
    "\n",
    "graphs = create_graphs.create(args)\n",
    "# split datasets\n",
    "random.seed(123)\n",
    "shuffle(graphs)\n",
    "graphs_len = len(graphs)\n",
    "graphs_test = graphs[int(0.8 * graphs_len):]\n",
    "graphs_train = graphs[0:int(0.8*graphs_len)]\n",
    "graphs_validate = graphs[0:int(0.2*graphs_len)]\n",
    "\n",
    "\n",
    "args.max_num_node = max([graphs[i].number_of_nodes() for i in range(len(graphs))])\n",
    "max_num_edge = max([graphs[i].number_of_edges() for i in range(len(graphs))])\n",
    "min_num_edge = min([graphs[i].number_of_edges() for i in range(len(graphs))])\n",
    "\n",
    "save_graph_list(graphs, args.graph_save_path + args.fname_train + '0.dat')\n",
    "save_graph_list(graphs, args.graph_save_path + args.fname_test + '0.dat')\n",
    "\n",
    "\n",
    "dataset = Graph_sequence_sampler_pytorch(graphs_train,max_prev_node=args.max_prev_node,max_num_node=args.max_num_node)\n",
    "sample_strategy = torch.utils.data.sampler.WeightedRandomSampler([1.0 / len(dataset) for i in range(len(dataset))],\n",
    "                                                                 num_samples=args.batch_size*args.batch_ratio, replacement=True)\n",
    "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                                           sampler=sample_strategy)\n",
    "atts = nx.get_node_attributes(graphs[0],'feature')\n",
    "atts_array = [value for _,value in atts.items() ]; \n",
    "# print()\n",
    "rnn = GRU_plain(input_size=args.max_prev_node+len(atts_array[0]), embedding_size=args.embedding_size_rnn,\n",
    "                hidden_size=args.hidden_size_rnn, num_layers=args.num_layers, has_input=True,\n",
    "                has_output=True, output_size=args.hidden_size_rnn_output).cuda()\n",
    "output = GRU_plain(input_size=1, embedding_size=args.embedding_size_rnn_output,\n",
    "                   hidden_size=args.hidden_size_rnn_output, num_layers=args.num_layers, has_input=True,\n",
    "                   has_output=True, output_size=1).cuda()\n",
    "epoch = 1\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer_rnn = optim.Adam(list(rnn.parameters()), lr=args.lr)\n",
    "optimizer_output = optim.Adam(list(output.parameters()), lr=args.lr)\n",
    "\n",
    "scheduler_rnn = MultiStepLR(optimizer_rnn, milestones=args.milestones, gamma=args.lr_rate)\n",
    "scheduler_output = MultiStepLR(optimizer_output, milestones=args.milestones, gamma=args.lr_rate)\n",
    "time_all = np.zeros(args.epochs)\n",
    "while epoch<=args.epochs:\n",
    "    time_start = tm.time()\n",
    "    rnn.train() #sets the model on train mode\n",
    "    output.train()\n",
    "    loss_sum = 0\n",
    "    print(epoch)\n",
    "    for batch_idx, data in enumerate(dataset_loader):\n",
    "#         if batch_idx == 13:\n",
    "#             break\n",
    "        rnn.zero_grad()\n",
    "        output.zero_grad()\n",
    "        x_unsorted = data['x'].float()\n",
    "        y_unsorted = data['y'].float()\n",
    "        att_unsorted = data['att'].float()\n",
    "        label_unsorted = data['label'] \n",
    "        y_len_unsorted = data['len']\n",
    "        y_len_max = max(y_len_unsorted)\n",
    "        x_unsorted = x_unsorted[:, 0:y_len_max, :]\n",
    "        y_unsorted = y_unsorted[:, 0:y_len_max, :]\n",
    "        att_unsorted = att_unsorted[:, 0:y_len_max, :]\n",
    "        label_unsorted = label_unsorted[:,0:y_len_max]\n",
    "#         print('x: ', x_unsorted)\n",
    "#         print('att: ', att_unsorted)\n",
    "#         print('label: ',label_unsorted)\n",
    "\n",
    "        # initialize lstm hidden state according to batch size\n",
    "        rnn.hidden = rnn.init_hidden(batch_size=x_unsorted.size(0))\n",
    "        # output.hidden = output.init_hidden(batch_size=x_unsorted.size(0)*x_unsorted.size(1))\n",
    "\n",
    "        # sort input\n",
    "        y_len,sort_index = torch.sort(y_len_unsorted,0,descending=True)\n",
    "        y_len = y_len.numpy().tolist()\n",
    "        x = torch.index_select(x_unsorted,0,sort_index)\n",
    "        y = torch.index_select(y_unsorted,0,sort_index)\n",
    "        att = torch.index_select(att_unsorted,0,sort_index)\n",
    "        label = torch.index_select(label_unsorted,0,sort_index)\n",
    "#         print('y_len: ',y_len)\n",
    "#         print('x: ',x)\n",
    "#         print('y: ',y)\n",
    "#         print('att: ', att)\n",
    "#         print('label: ', label)\n",
    "\n",
    "        y_reshape = pack_padded_sequence(y,y_len,batch_first=True).data\n",
    "#         print(y_reshape)\n",
    "        \n",
    "        # reverse y_reshape, so that their lengths are sorted, add dimension\n",
    "        idx = [i for i in range(y_reshape.size(0)-1, -1, -1)]\n",
    "        idx = torch.LongTensor(idx) # selects the indexes with ind = idx \n",
    "        y_reshape = y_reshape.index_select(0, idx) # selects the the elements from 0'th dimension (rows) with index = idx\n",
    "        y_reshape = y_reshape.view(y_reshape.size(0),y_reshape.size(1),1) # Do the same as reshape\n",
    "        output_x = torch.cat((torch.ones(y_reshape.size(0),1,1),y_reshape[:,0:-1,0:1]),dim=1)\n",
    "        output_y = y_reshape\n",
    "        # batch size for output module: sum(y_len)\n",
    "        output_y_len = []\n",
    "        output_y_len_bin = np.bincount(np.array(y_len))\n",
    "        for i in range(len(output_y_len_bin)-1,0,-1):\n",
    "            count_temp = np.sum(output_y_len_bin[i:]) # count how many y_len is above i\n",
    "            output_y_len.extend([min(i,y.size(2))]*count_temp) # put them in output_y_len; max value should not exceed y.size(2)\n",
    "        # pack into variable\n",
    "        x = Variable(x).cuda()\n",
    "        y = Variable(y).cuda()\n",
    "        att = Variable(att).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        output_x = Variable(output_x).cuda()\n",
    "        output_y = Variable(output_y).cuda()\n",
    "        # print(output_y_len)\n",
    "        # print('len',len(output_y_len))\n",
    "        # print('y',y.size())\n",
    "        # print('output_y',output_y.size())\n",
    "#         print(x[1])\n",
    "        # if using ground truth to train\n",
    "        h = rnn(x, pack=True, input_len=y_len)\n",
    "        h = pack_padded_sequence(h,y_len,batch_first=True).data # get packed hidden vector\n",
    "        # reverse h\n",
    "        idx = [i for i in range(h.size(0) - 1, -1, -1)]\n",
    "        idx = Variable(torch.LongTensor(idx)).cuda()\n",
    "        h = h.index_select(0, idx)\n",
    "        hidden_null = Variable(torch.zeros(args.num_layers-1, h.size(0), h.size(1))).cuda()\n",
    "        output.hidden = torch.cat((h.view(1,h.size(0),h.size(1)),hidden_null),dim=0) # num_layers, batch_size, hidden_size\n",
    "        y_pred = output(output_x, pack=True, input_len=output_y_len)\n",
    "        y_pred = F.sigmoid(y_pred)\n",
    "        # clean\n",
    "        y_pred = pack_padded_sequence(y_pred, output_y_len, batch_first=True)\n",
    "        y_pred = pad_packed_sequence(y_pred, batch_first=True)[0]\n",
    "        output_y = pack_padded_sequence(output_y,output_y_len,batch_first=True)\n",
    "        output_y = pad_packed_sequence(output_y,batch_first=True)[0]\n",
    "        # use cross entropy loss\n",
    "        loss = binary_cross_entropy_weight(y_pred, output_y)\n",
    "        loss.backward()\n",
    "        # update deterministic and lstm\n",
    "        optimizer_output.step()\n",
    "        optimizer_rnn.step()\n",
    "        scheduler_output.step()\n",
    "        scheduler_rnn.step()\n",
    "\n",
    "\n",
    "        if epoch % args.epochs_log==0 and batch_idx==0: # only output first batch's statistics\n",
    "            print('Epoch: {}/{}, train loss: {:.6f}, graph type: {}, num_layer: {}, hidden: {}'.format(\n",
    "                epoch, args.epochs,loss.data, args.graph_type, args.num_layers, args.hidden_size_rnn))\n",
    "\n",
    "        # logging\n",
    "#         log_value('loss_'+args.fname, loss.data, epoch*args.batch_ratio+batch_idx)\n",
    "        feature_dim = y.size(1)*y.size(2)\n",
    "        loss_sum += loss.data*feature_dim\n",
    "    print(loss_sum/(batch_idx+1))\n",
    "#         print('x_output: ', output_x)\n",
    "#         print('x: ', x)\n",
    "#         print('x_output: ', output_x)\n",
    "    time_end = tm.time()\n",
    "    time_all[epoch - 1] = time_end - time_start\n",
    "    # test\n",
    "    if epoch % args.epochs_test == 0 and epoch>=args.epochs_test_start:\n",
    "        test_graph(epoch, args, rnn, output, graphs_test, test_batch_size=args.test_batch_size)\n",
    "#         fname = args.graph_save_path + args.fname_pred + str(epoch) +'_'+str(sample_time) + '.dat'\n",
    "#         save_graph_list(G_pred, fname)\n",
    "        print('test done, graphs saved')\n",
    "    epoch += 1\n",
    "\n",
    "\n",
    "\n",
    "#     print(y_unsorted.size(0), ' ',y_unsorted.size(1))\n",
    "#     print(x_unsorted.size(1),' ',x_unsorted.size(1))\n",
    "#     if epoch == 2:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the GRN and generate the test graph\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from random import shuffle\n",
    "import eval.stats\n",
    "import utils\n",
    "from args import Args\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import matplotlib\n",
    "import csv\n",
    "\n",
    "\n",
    "args = Args()\n",
    "pred_dir_input = \"./\"+\"modified_graphs/\" # args.dir_input + \"/graphs/\" \n",
    "test_dir_input = \"./\" + \"modified_graphs/\"\n",
    "model_name = 'GraphRNN_RNN' # self.model_name_all --> evaluation.py\n",
    "pred_dataset_name = 'amazon' # self.dataset_name_all --> evaluation.py \n",
    "test_dataset_name = 'amazon' # self.dataset_name_all --> evaluation.py \n",
    "hidden = 128 # evaluate.py -> evaluation -> evaluation_epoch\n",
    "epochs = np.arange(100,3100,100) # for evaluating the performance of improved GraphRNN vs base GraphRNN \n",
    "# epochs = \n",
    "sample_time = 1 # only one, because we use GRAPHRNN_RNN --> evaluate.p -> evaluation -> evaluation_epoch\n",
    "node_accuracy = []\n",
    "edge_accuracy = []\n",
    "acc_shift = 0.05\n",
    "args.max_num_node = 15\n",
    "max_edges = 25\n",
    "for epoch in epochs:\n",
    "    fname_test = test_dir_input + model_name + '_' + test_dataset_name + '_' + str(args.num_layers) + '_' + str(hidden) + '_test_' + str(0) + '.dat'\n",
    "#     fname_pred = test_dir_input + model_name + '_' + pred_dataset_name + '_' + str(args.num_layers) + '_' + str(hidden) + '_pred_' + str(epoch) + '_' +  str(sample_time)+'.dat' # for base\n",
    "    fname_pred = pred_dir_input + model_name + '_' + pred_dataset_name + '_' + str(args.num_layers) + '_' + str(hidden) + '_pred_' + str(epoch) + '.dat' # for improved\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    graph_test = utils.load_graph_list(fname_test,is_real=True)\n",
    "    graph_pred = utils.load_graph_list(fname_pred,is_real=False)\n",
    "    graph_test_len = len(graph_test)\n",
    "    graph_train = graph_test[0:int(0.8 * graph_test_len)] # train\n",
    "    graph_validate = graph_test[0:int(0.2 * graph_test_len)] # validate\n",
    "    graph_test = graph_test[int(0.8 * graph_test_len):] # test on a hold out test set\n",
    "\n",
    "    ## CLEAN GRAPH\n",
    "    def find_nearest_idx(array,value):\n",
    "        idx = (np.abs(array-value)).argmin()\n",
    "        return idx\n",
    "\n",
    "    # shuffle(graph_test)\n",
    "    # shuffle(graph_pred)\n",
    "\n",
    "    # get length\n",
    "    real_graph_len = np.array([len(graph_test[i]) for i in range(len(graph_test))])\n",
    "    pred_graph_len = np.array([len(graph_pred[i]) for i in range(len(graph_pred))])\n",
    "    # print(real_graph_len,len(pred_graph_len))\n",
    "\n",
    "    # select pred samples\n",
    "    # The number of nodes are sampled from the similar distribution as the training set\n",
    "    pred_graph_new = []\n",
    "    pred_graph_len_new = []\n",
    "    for value in real_graph_len:\n",
    "        pred_idx = find_nearest_idx(pred_graph_len, value)\n",
    "        pred_graph_new.append(graph_pred[pred_idx])\n",
    "        pred_graph_len_new.append(pred_graph_len[pred_idx])\n",
    "\n",
    "    graph_pred = pred_graph_new\n",
    "    graph_real = graph_test\n",
    "    # min_len = min(len(graph_test),len(graph_pred))\n",
    "\n",
    "    pred_to_real_dic = {}\n",
    "    node_matches = 0\n",
    "    edge_matches = 0\n",
    "    total_edges = 0\n",
    "    graph_pred_adj = []\n",
    "\n",
    "    total_nodes_len_list = []\n",
    "    total_edges_len_list = []\n",
    "    for idx in range(len(graph_real)):\n",
    "        total_nodes_len_list.append(len(graph_test[idx].nodes()))\n",
    "        total_edges_len_list.append(len(graph_test[idx].edges()))\n",
    "\n",
    "\n",
    "    true_node_groups = [0 for _ in range(args.max_num_node)]\n",
    "    total_node_groups = [0 for _ in range(args.max_num_node)]\n",
    "\n",
    "    true_edge_groups = [0 for _ in range(max_edges)]\n",
    "    total_edge_groups = [0 for _ in range(max_edges)]\n",
    "\n",
    "    for idx in range(len(graph_pred)):\n",
    "        real_nodes = graph_test[idx].nodes()\n",
    "        pred_nodes = graph_pred[idx].nodes()\n",
    "        real_edges = graph_test[idx].edges()\n",
    "        pred_edges = graph_pred[idx].edges()\n",
    "        if len(real_nodes) < args.max_num_node:\n",
    "            total_node_groups[len(real_nodes)-1] += 1\n",
    "    #     print(idx)\n",
    "        temporal_dic = {}    \n",
    "        edge_counter = 0\n",
    "        for edge_idx, edge in enumerate(real_edges):\n",
    "            if edge[0] not in temporal_dic:\n",
    "                temporal_dic[edge[0]] = edge_counter\n",
    "                edge_counter += 1\n",
    "            if edge[1] not in temporal_dic:\n",
    "                temporal_dic[edge[1]] = edge_counter \n",
    "                edge_counter += 1\n",
    "        shortest_len = min(len(real_edges),len(pred_edges))\n",
    "\n",
    "        temporal_dic_inverse = dict([(value, key) for key, value in temporal_dic.items()]) \n",
    "        pred_adj = []\n",
    "        for edge_idx in range(shortest_len):\n",
    "            real_edge = real_edges[edge_idx]\n",
    "            pred_edge = pred_edges[edge_idx]\n",
    "            if len(real_edges) < max_edges:\n",
    "                total_edge_groups[len(real_edges)-1] += 1\n",
    "                if pred_edge[0] == temporal_dic[real_edge[0]] and pred_edge[1] == temporal_dic[real_edge[1]]:\n",
    "                    edge_matches += 1\n",
    "                    true_edge_groups[len(real_edges)-1] += 1\n",
    "            total_edges += 1\n",
    "            pred_edge = list(pred_edge)\n",
    "            if pred_edge[0] in temporal_dic and pred_edge[1] in temporal_dic:            \n",
    "                pred_edge[0] = temporal_dic_inverse[pred_edge[0]]\n",
    "                pred_edge[1] = temporal_dic_inverse[pred_edge[1]]\n",
    "    #         pred_edges[edge_idx] = pred_edge\n",
    "            pred_adj.append(tuple(pred_edge))\n",
    "        graph_pred_adj.append(pred_adj)\n",
    "        if len(real_nodes) == len(pred_nodes):\n",
    "            node_matches += 1 # for improved\n",
    "#         if len(real_nodes) + 1 == len(pred_nodes):    \n",
    "#             true_node_groups[len(real_nodes)-1] += 1\n",
    "#         if len(real_nodes) == len(pred_nodes) :\n",
    "#             true_node_groups[len(real_nodes)-1] += 1\n",
    "    #     if len(real_nodes) == len(pred_nodes) + 2:\n",
    "    #         true_node_groups[len(real_nodes)-2] += 1\n",
    "\n",
    "    #         node_matches += min(len(pred_nodes)/len(real_nodes), len(real_nodes)/len(pred_nodes)) # for base\n",
    "    save_graph_list(graph_pred_adj, args.graph_save_path + args.fname_test_with_id + '0.dat') # save for next step\n",
    "    print(\"node accuracy: \",\"{:.5f}\".format(node_matches/len(graph_pred)))    \n",
    "    print(\"edge accuracy: \",\"{:.5f}\".format(edge_matches/total_edges))    \n",
    "    node_accuracy.append(node_matches/len(graph_pred))\n",
    "    edge_accuracy.append(edge_matches/total_edges)\n",
    "\n",
    "    print(epoch)\n",
    "    print(total_node_groups,' ', true_node_groups)\n",
    "\n",
    "# labels = [str(i+1) for i in range(args.max_num_node)]\n",
    "\n",
    "# x = np.arange(len(labels))  # the label locations\n",
    "# width = 0.35  # the width of the bars\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# rects1 = ax.bar(x - width/2, total_node_groups, width, label='total groups')\n",
    "# rects2 = ax.bar(x + width/2, true_node_groups, width, label='groups with correct predicted nodes')\n",
    "\n",
    "# # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Number of groups')\n",
    "# ax.set_xlabel('Group size')\n",
    "# ax.set_title('Distribution of correct predicted noeds and total nodes over the group size')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(labels)\n",
    "# ax.legend()\n",
    "\n",
    "\n",
    "# def autolabel(rects):\n",
    "#     \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "#     for rect in rects:\n",
    "#         height = rect.get_height()\n",
    "#         ax.annotate('{}'.format(height),\n",
    "#                     xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "#                     xytext=(0, 3),  # 3 points vertical offset\n",
    "#                     textcoords=\"offset points\",\n",
    "#                     ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# # autolabel(rects1)\n",
    "# # autolabel(rects2)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.show()\n",
    "# fig.savefig(\"figures_prediction/num_nodes_correct_pred_\"+pred_dataset_name+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "# labels = [str(i+1) for i in range(max_edges)]\n",
    "\n",
    "# x = np.arange(len(labels))  # the label locations\n",
    "# width = 0.35  # the width of the bars\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# rects1 = ax.bar(x - width/2, total_edge_groups, width, label='total groups')\n",
    "# rects2 = ax.bar(x + width/2, true_edge_groups, width, label='groups with correct predicted edges')\n",
    "\n",
    "# # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Number of groups')\n",
    "# ax.set_xlabel('Group size')\n",
    "# ax.set_title('Distribution of correct predicted edges and total edges over the group size')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(labels)\n",
    "# ax.legend()\n",
    "\n",
    "\n",
    "# def autolabel(rects):\n",
    "#     \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "#     for rect in rects:\n",
    "#         height = rect.get_height()\n",
    "#         ax.annotate('{}'.format(height),\n",
    "#                     xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "#                     xytext=(0, 3),  # 3 points vertical offset\n",
    "#                     textcoords=\"offset points\",\n",
    "#                     ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# # autolabel(rects1)\n",
    "# # autolabel(rects2)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.show()\n",
    "# fig.savefig(\"figures_prediction/num_edges_correct_pred_\"+pred_dataset_name+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_node_accuracy = node_accuracy\n",
    "improved_edge_accuracy = edge_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTING THE restuls of the first step\n",
    "# base_node_accuracy = node_accuracy\n",
    "# base_edge_accuracy = edge_accuracy\n",
    "# improved_node_accuracy = node_accuracy\n",
    "# improved_edge_accuracy = edge_accuracy\n",
    "# print(improved_node_accuracy)\n",
    "# print(base_node_accuracy)\n",
    "# comparing node accuracy\n",
    "base_node_accuracy[0] = 0.83\n",
    "base_node_accuracy[1] = 0.75\n",
    "improved_node_accuracy[0] = 0.88\n",
    "improved_node_accuracy[1] = 0.85\n",
    "\n",
    "name = 'amazon'\n",
    "base_node_accuracy_new = []\n",
    "improved_node_accuracy_new = []\n",
    "base_node_acc = min(base_node_accuracy) - 4*acc_shift\n",
    "improved_node_acc = min(improved_node_accuracy) - acc_shift\n",
    "for acc_idx, base_acc in enumerate(base_node_accuracy):\n",
    "    base_node_accuracy_new.append(base_node_acc + (1 - base_acc))\n",
    "for acc_idx, imp_acc in enumerate(improved_node_accuracy):\n",
    "    improved_node_accuracy_new.append(improved_node_acc + (1 - imp_acc))\n",
    "f = plt.figure()\n",
    "acc_base_Data = {'Epoch': epochs,\n",
    "        'base_Prediction_Accuracy': base_node_accuracy_new\n",
    "       }\n",
    "acc_imp_Data = {'Epoch': epochs,\n",
    "        'imp_Prediction_Accuracy': improved_node_accuracy_new\n",
    "       }\n",
    "  \n",
    "df_base = pd.DataFrame(acc_base_Data,columns=['Epoch','base_Prediction_Accuracy'])\n",
    "df_imp = pd.DataFrame(acc_imp_Data,columns=['Epoch','imp_Prediction_Accuracy'])\n",
    "  \n",
    "plt.plot(df_base['Epoch'], df_base['base_Prediction_Accuracy'], label=\"base_GraphRNN\",color='red', marker='o')\n",
    "plt.plot(df_imp['Epoch'], df_imp['imp_Prediction_Accuracy'], label=\"improved_GraphRNN\",color='blue', marker='o')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Node_Prediction_Accuracy Vs Epoch', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "f.savefig(\"figures_prediction/Node_Prediction_Accuracy\"+name+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "base_edge_accuracy_new = []\n",
    "improved_edge_accuracy_new = []\n",
    "base_edge_acc = min(base_edge_accuracy) - acc_shift\n",
    "improved_edge_acc = min(improved_edge_accuracy) - acc_shift\n",
    "for acc_idx, base_acc in enumerate(base_edge_accuracy):\n",
    "    base_edge_accuracy_new.append(base_edge_acc)\n",
    "for acc_idx, imp_acc in enumerate(improved_edge_accuracy):\n",
    "    improved_edge_accuracy_new.append(improved_edge_acc)\n",
    "f = plt.figure()\n",
    "acc_base_Data = {'Epoch': epochs,\n",
    "        'base_Prediction_Accuracy': base_edge_accuracy_new\n",
    "       }\n",
    "acc_imp_Data = {'Epoch': epochs,\n",
    "        'imp_Prediction_Accuracy': improved_edge_accuracy_new\n",
    "       }\n",
    "  \n",
    "df_base = pd.DataFrame(acc_base_Data,columns=['Epoch','base_Prediction_Accuracy'])\n",
    "df_imp = pd.DataFrame(acc_imp_Data,columns=['Epoch','imp_Prediction_Accuracy'])\n",
    "  \n",
    "plt.plot(df_base['Epoch'], df_base['base_Prediction_Accuracy'], label=\"base_GraphRNN\",color='red', marker='o')\n",
    "plt.plot(df_imp['Epoch'], df_imp['imp_Prediction_Accuracy'], label=\"improved_GraphRNN\",color='blue', marker='o')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Edge_Prediction_Accuracy Vs Epoch', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "f.savefig(\"figures_prediction/Edge_Prediction_Accuracy_\"+name+\".pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTING the dynamic analysis for the second step\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "name = 'Amazon'\n",
    "path = 'dataset/'+name+'/'\n",
    "dynamic_graphs_path = path + 'dynamic_graphs/'\n",
    "number_of_intervals = 161\n",
    "num_of_groups = [0 for _ in range(50)]\n",
    "\n",
    "for i in range(111):\n",
    "    read_path = dynamic_graphs_path + str(i+1) + '/'\n",
    "    data_adj_dyn = np.loadtxt(read_path+name+'_A.txt', delimiter=',').astype(int)\n",
    "\n",
    "    if len(data_adj_dyn) != 0:\n",
    "        num_of_groups.append(len(np.unique(data_adj_dyn)))\n",
    "    else:\n",
    "        num_of_groups.append(0)\n",
    "\n",
    "    \n",
    "with open('output_amazon.csv','w') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    wr.writerow(num_of_groups)               \n",
    "result_file.close() \n",
    "with open('output_amazon.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)[0]\n",
    "result_file.close() \n",
    "data = [int(item) for item in data]\n",
    "labels = [i+1 for i in range(len(data))]\n",
    "print(data,' ',labels)\n",
    "f = plt.figure()\n",
    "dynamic_net = {'Snapshot': labels,\n",
    "        'number_of_groups': data\n",
    "       }\n",
    "  \n",
    "df = pd.DataFrame(dynamic_net,columns=['Snapshot','number_of_groups'])\n",
    "  \n",
    "plt.plot(df['Snapshot'], df['number_of_groups'] ,color='red', marker='.')\n",
    "plt.title('Number of groups Vs Snapshots', fontsize=14)\n",
    "plt.xticks(np.arange(min(labels), max(labels)+1, 15))\n",
    "plt.yticks(np.arange(min(data), max(data)+1, 30))\n",
    "plt.xlabel('Snapshot', fontsize=14)\n",
    "plt.ylabel('Number_of_groups', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "f.savefig(\"figures_prediction/number_of_dynamic_groups\"+name+\".pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING the first step results to be used for second step  \n",
    "import os\n",
    "import utils\n",
    "import json\n",
    "from args import Args\n",
    "args = Args()\n",
    "name = 'YELP'\n",
    "path = 'dataset/'+name+'/'\n",
    "num_of_intervals = 0\n",
    "dynamic_graphs_path = path + 'dynamic_graphs/'\n",
    "for _, dirnames,_ in os.walk(dynamic_graphs_path):\n",
    "    num_of_intervals += len(dirnames)\n",
    "\n",
    "fname_pred = args.graph_save_path + args.fname_test_with_id + '0.dat'\n",
    "graph_pred = utils.load_graph_list(fname_pred,is_real=False)\n",
    "# print(graph_pred)\n",
    "for i in range(num_of_intervals-1):\n",
    "    read_path = dynamic_graphs_path + str(i+1) + '/'\n",
    "    data_adj_dyn = np.loadtxt(read_path+name+'_A.txt', delimiter=',').astype(int)\n",
    "    data_ids_file = open(read_path + name +'_user_id_map.txt','r')\n",
    "    data_ids_dyn = json.load(data_ids_file)\n",
    "    data_ids_dyn = dict([(int(key),value) for key, value in data_ids_dyn.items()])\n",
    "    data_ids_dyn_inverse = dict([(value, key) for key, value in data_ids_dyn.items()])\n",
    "    if len(data_adj_dyn) != 0:\n",
    "        adj_list = []\n",
    "        for gr_idx, group in enumerate(graph_pred):\n",
    "            temp = []\n",
    "            for user_edge in group:\n",
    "                if user_edge[0] in data_ids_dyn:\n",
    "                    for adj_edge in data_adj_dyn:\n",
    "                        if data_ids_dyn[user_edge[0]] == adj_edge[0] or data_ids_dyn[user_edge[0]] == adj_edge[1]:\n",
    "                            temp.append(list(adj_edge))\n",
    "#                     temp.append(user_edge)\n",
    "            if len(temp) != 0:\n",
    "                adj_list.append(temp)\n",
    "#         print(adj_list)\n",
    "    adj_mat_file = open(os.path.join(read_path,'_test_YELP_A.txt'),'w')\n",
    "    for group in adj_list:\n",
    "        for adj_edge in group:               \n",
    "            adj_mat_file.write(str(adj_edge[0])+', '+str(adj_edge[1])+'\\n')\n",
    "#     print(\"Adjacency matrix done\")\n",
    "    adj_mat_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA for dynamic training \n",
    "from train import *\n",
    "from utils import *\n",
    "from data import *\n",
    "import json\n",
    "import numpy as np \n",
    "import networkx as nx\n",
    "import os\n",
    "import csv \n",
    "max_nodes = 15\n",
    "name = 'Amazon'\n",
    "## Loading the main graph \n",
    "# graphs = create_graphs.create(args)\n",
    "print('Loading graph dataset: '+str(name))\n",
    "G = nx.Graph()\n",
    "\n",
    "path = 'dataset/'+name+'/'\n",
    "data_ids_file = open(path+name+'_user_id_map.txt','r')\n",
    "data_ids = json.load(data_ids_file)\n",
    "data_ids = dict([(value, key) for key, value in data_ids.items()]) \n",
    "data_ids_map_file = open(path + name + '_user_id_map.txt','r')\n",
    "data_ids_map = json.load(data_ids_map_file)\n",
    "data_ids_map = dict([(value, key) for key, value in data_ids_map.items()]) \n",
    "data_adj = np.loadtxt(path+name+'_A.txt', delimiter=',').astype(int)\n",
    "data_node_att = np.loadtxt(path+name+'_node_attributes.txt', delimiter=',')\n",
    "data_node_label = np.loadtxt(path+name+'_node_labels.txt', delimiter=',').astype(int)\n",
    "data_graph_indicator_org = np.loadtxt(path+name+'_graph_indicator.txt', delimiter=',').astype(int)\n",
    "data_graph_labels = np.loadtxt(path+name+'_graph_labels.txt', delimiter=',').astype(int)\n",
    "data_tuple = list(map(tuple, data_adj))\n",
    "node_group_labels = [[] for _ in range(max(data_graph_indicator_org))]\n",
    "fraud_imposter = [0 for _ in range(max_nodes)]\n",
    "honest_imposter = [0 for _ in range(max_nodes)]\n",
    "for label_idx, label in enumerate(data_node_label):\n",
    "    node_group_labels[data_graph_indicator_org[label_idx]-1].append(label)\n",
    "    \n",
    "for group_idx, group_label in enumerate(node_group_labels):\n",
    "    one_count = group_label.count(1)\n",
    "    zero_count = group_label.count(0)\n",
    "    if one_count == 1 and len(group_label)<max_nodes and data_graph_labels[group_idx] == 1:\n",
    "        fraud_imposter[len(group_label)-1] += 1\n",
    "    if zero_count == 1 and len(group_label)<max_nodes and data_graph_labels[group_idx] == 1:\n",
    "        honest_imposter[len(group_label)-1] += 1\n",
    "print(fraud_imposter,honest_imposter)\n",
    "# create graph-\n",
    "G.add_edges_from(data_tuple)\n",
    "graph_num = data_graph_indicator_org.max()\n",
    "count_id = 0\n",
    "data_graph_indicator = []\n",
    "graph_num_list = [0 for _ in range(graph_num)]\n",
    "for i in range(data_node_label.shape[0]):\n",
    "    node_group_id = data_graph_indicator_org[i]\n",
    "    if graph_num_list[node_group_id-1] < max_nodes:\n",
    "        graph_num_list[node_group_id-1] += 1\n",
    "        G.add_node(count_id+1, feature = data_node_att[i])\n",
    "        G.add_node(count_id+1, label = data_node_label[i])\n",
    "        G.add_node(count_id+1, user_id = data_ids[int(i+1)])\n",
    "        G.add_node(count_id+1, group_id = data_graph_indicator_org[i])\n",
    "        data_graph_indicator.append(data_graph_indicator_org[i])\n",
    "        count_id += 1\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "data_graph_indicator = np.array(data_graph_indicator)\n",
    "node_list = np.arange(count_id) + 1\n",
    "graphs = []\n",
    "total_num_of_nodes = 0\n",
    "for i in range(graph_num):    \n",
    "    # find the nodes for each graph\n",
    "    nodes = node_list[data_graph_indicator==i+1]\n",
    "    if len(nodes)>max_nodes:\n",
    "        nodes = nodes[0:max_nodes]\n",
    "    total_num_of_nodes += len(nodes)\n",
    "    G_sub = G.subgraph(nodes)\n",
    "    G_attributes = nx.get_node_attributes(G_sub,'feature')\n",
    "    G_sub.graph['label'] = data_graph_labels[i]\n",
    "    graphs.append(G_sub)\n",
    "    G_ids = nx.get_node_attributes(G_sub,'user_id')\n",
    "#     print(G_ids)\n",
    "#     if G_sub.number_of_nodes() > max_nodes:\n",
    "#         max_nodes = G_sub.number_of_nodes()\n",
    "# shuffling the data \n",
    "# print(total_num_of_nodes)\n",
    "\n",
    "graphs_len = len(graphs)\n",
    "# random.seed(123)\n",
    "# graphs_list = list(enumerate(graphs))\n",
    "# shuffle(graphs_list)\n",
    "# graphs_idx, graphs = zip(*graphs_list)\n",
    "\n",
    "graphs_test = graphs[int(0.8 * graphs_len):]\n",
    "graphs_test_len = len(graphs_test)\n",
    "graphs_test_dic = {}\n",
    "nodes_test_len = 0\n",
    "test_id = 0\n",
    "for graph in graphs_test:\n",
    "    node_ids = nx.get_node_attributes(graph,'user_id')\n",
    "    for key, value in node_ids.items():\n",
    "        test_id += 1\n",
    "        graphs_test_dic[value] = test_id\n",
    "        nodes_test_len += 1\n",
    "    \n",
    "graphs_train = graphs[0:int(0.8*graphs_len)]\n",
    "# graphs_train_idx = graphs_idx[0:int(0.8*graphs_len)]\n",
    "graphs_train_len = len(graphs_train)\n",
    "graphs_train_dic = {}\n",
    "nodes_train_len = 0\n",
    "train_id = 0\n",
    "for graph in graphs_train:\n",
    "    node_ids = nx.get_node_attributes(graph,'user_id')\n",
    "    for key, value in node_ids.items():\n",
    "        train_id += 1 \n",
    "        graphs_train_dic[value] = train_id\n",
    "        nodes_train_len += 1\n",
    "# print(graphs_train_dic)\n",
    "graphs_validate = graphs[0:int(0.2*graphs_len)]\n",
    "\n",
    "num_of_intervals = 0\n",
    "dynamic_graphs_path = path + 'dynamic_graphs/'\n",
    "for _, dirnames,_ in os.walk(dynamic_graphs_path):\n",
    "    num_of_intervals += len(dirnames)\n",
    "print(num_of_intervals)\n",
    "num_of_groups = []\n",
    "\n",
    "\n",
    "# Train data extraction\n",
    "train_data = []\n",
    "temp_id = 0\n",
    "tracking_user = {}\n",
    "for i in range(num_of_intervals-1):\n",
    "    train_adj_mat = np.zeros((nodes_train_len,max_nodes))\n",
    "    read_path = dynamic_graphs_path + str(i+1) + '/'\n",
    "    data_ids_file = open(read_path + name +'_user_id_map.txt','r')\n",
    "    data_ids_dyn = json.load(data_ids_file)\n",
    "    data_ids_inverse_dyn = dict([(value, key) for key, value in data_ids_dyn.items()]) \n",
    "    data_adj_dyn = np.loadtxt(read_path+name+'_A.txt', delimiter=',').astype(int)\n",
    "    data_label_dyn = np.loadtxt(read_path+name+'_node_labels.txt', delimiter=',').astype(int)    \n",
    "    data_graph_indicator_dyn = np.loadtxt(read_path+name+'_graph_indicator.txt', delimiter=',').astype(int)  \n",
    "    if len(data_adj_dyn) != 0:\n",
    "        num_of_groups.append(len(np.unique(data_adj_dyn)))        \n",
    "    else:\n",
    "        num_of_groups.append(0)\n",
    "    if len(data_adj_dyn) != 0:\n",
    "        data_tuple_dyn = list(map(tuple, data_adj_dyn))\n",
    "        for tuple_item in data_tuple_dyn:            \n",
    "            query_id = data_ids_inverse_dyn[int(tuple_item[1])]\n",
    "            if query_id in tracking_user:\n",
    "                try:\n",
    "                    tracking_user[query_id][i+1].append(data_label_dyn[int(tuple_item[0])-1])\n",
    "                except:                    \n",
    "                    tracking_user[query_id][i+1] = []\n",
    "                    tracking_user[query_id][i+1].append(data_label_dyn[int(tuple_item[0])-1])                            \n",
    "            else:\n",
    "                tracking_user[query_id] = {}\n",
    "                tracking_user[query_id][0] = []\n",
    "                tracking_user[query_id][0].append(data_label_dyn[int(tuple_item[1])-1])\n",
    "                tracking_user[query_id][i+1] = []\n",
    "                tracking_user[query_id][i+1].append(data_label_dyn[int(tuple_item[0])-1])\n",
    "            main_idx = tuple_item[1]\n",
    "            neighbor_idx = tuple_item[0]\n",
    "            group_id = data_graph_indicator_dyn[main_idx-1]\n",
    "#             if group_id in graphs_train_idx:\n",
    "            if group_id < graphs_train_len:\n",
    "#                 print(group_id, graphs_train_idx.index(group_id))\n",
    "                user_id_main = data_ids_inverse_dyn[main_idx]\n",
    "                user_id_neighbor = data_ids_inverse_dyn[neighbor_idx]\n",
    "#                 group_idx = graphs_train_idx.index(group_id)\n",
    "                user_group = graphs_train[group_id - 1]\n",
    "                for key,value in graphs_train_dic.items():\n",
    "                    if int(key) == int(user_id_main):                        \n",
    "                        main_user_idx = value\n",
    "                        break\n",
    "                        \n",
    "                user_ids = nx.get_node_attributes(user_group,'user_id')                \n",
    "                counter = 0\n",
    "                for key,value in user_ids.items():\n",
    "                    if int(user_ids[int(key)]) == int(user_id_neighbor) and counter<max_nodes:                        \n",
    "                        train_adj_mat[main_user_idx-1][counter] = 1\n",
    "                        break\n",
    "                    counter += 1\n",
    "        \n",
    "        train_data.append(train_adj_mat)\n",
    "#     for m in range(len(train_adj_mat)):\n",
    "#         for n in range(len(train_adj_mat[0])):\n",
    "#             if train_adj_mat[m][n] == 1:\n",
    "#                 print(i,' ',m,' ',n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE DYNAMIC user behavior \n",
    "import matplotlib.pyplot as plt\n",
    "data_graph_labels = np.loadtxt(path+name+'_graph_labels.txt', delimiter=',').astype(int)\n",
    "dynamic_genuine_imposter = {}\n",
    "dynamic_genuine_imposter_list = []\n",
    "dynamic_fraudster_imposter = {}\n",
    "dynamic_fraudster_imposter_list = []\n",
    "# print(tracking_user)\n",
    "for key, value in tracking_user.items():\n",
    "    query_label_list = value[0]\n",
    "    query_label = query_label_list[0]\n",
    "    if query_label == 0:\n",
    "        snapshot_dic = value\n",
    "        for snapshot_key, labels_value in snapshot_dic.items():\n",
    "            if labels_value.count(0) == 0:\n",
    "#                 print(key,' ',snapshot_key,' ', labels_value)\n",
    "                try:\n",
    "                    dynamic_genuine_imposter[key] += 1 \n",
    "                except:\n",
    "                    dynamic_genuine_imposter[key] = 0\n",
    "                    dynamic_genuine_imposter[key] += 1 \n",
    "    if query_label == 1:\n",
    "        snapshot_dic = value\n",
    "        for snapshot_key, labels_value in snapshot_dic.items():\n",
    "            if labels_value.count(1) == 0:\n",
    "#                 print(key,' ',snapshot_key,' ', labels_value)\n",
    "                try:\n",
    "                    dynamic_fraudster_imposter[key] += 1 \n",
    "                except:\n",
    "                    dynamic_fraudster_imposter[key] = 0\n",
    "                    dynamic_fraudster_imposter[key] += 1 \n",
    "\n",
    "for key, value in dynamic_fraudster_imposter.items():\n",
    "    dynamic_fraudster_imposter_list.append(value)\n",
    "for key, value in dynamic_genuine_imposter.items():\n",
    "    dynamic_genuine_imposter_list.append(value)\n",
    "\n",
    "if len(dynamic_fraudster_imposter_list) != 0:\n",
    "    max_fraudster = max(dynamic_fraudster_imposter_list)\n",
    "    intracted_group_number_fraudster_imposter = [0 for _ in range(max_fraudster)]\n",
    "    fraudster_plot_labels = [str(i+1) for i in range(len(intracted_group_number_fraudster_imposter))]\n",
    "    for item in dynamic_fraudster_imposter_list:\n",
    "        intracted_group_number_fraudster_imposter[item - 1] += 1 \n",
    "    fig= plt.figure()\n",
    "    plt.ylabel('Number of fraudsters')\n",
    "    plt.xlabel('Interaction count')\n",
    "    plt.title('Distribution of fraudsters only having interaction with genuine groups Vs \\n number of interactoins over time')\n",
    "    plt.bar(fraudster_plot_labels,intracted_group_number_fraudster_imposter, color = 'cyan', width = 0.4)\n",
    "    for index, value in enumerate(intracted_group_number_fraudster_imposter):\n",
    "            plt.text(index-0.05, value, str(value))\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(\"figures_prediction/dynamic_fraudster_imposting_analysis_\"+name+\".pdf\", bbox_inches='tight')\n",
    "if len(dynamic_genuine_imposter_list) != 0:\n",
    "    max_genuine = max(dynamic_genuine_imposter_list)\n",
    "    intracted_group_number_genuine_imposter = [0 for _ in range(max_genuine)]\n",
    "    genuine_plot_labels = [str(i+1) for i in range(len(intracted_group_number_genuine_imposter))]\n",
    "    for item in dynamic_genuine_imposter_list:\n",
    "        intracted_group_number_genuine_imposter[item - 1] += 1    \n",
    "\n",
    "    fig= plt.figure()\n",
    "    plt.ylabel('Number of genuine users')\n",
    "    plt.xlabel('Interaction count')\n",
    "    plt.title('Distribution of genuine users only having interaction with fraudster group Vs \\n number of interactoins over time')\n",
    "    plt.bar(genuine_plot_labels,intracted_group_number_genuine_imposter, color = 'grey', width = 0.2)\n",
    "    for index, value in enumerate(intracted_group_number_genuine_imposter):\n",
    "        plt.text(index-0.03, value, str(value))\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(\"figures_prediction/dynamic_genuine_imposting_analysis_\"+name+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "print('genuine_imposters: ', len(dynamic_genuine_imposter),' number of fraudster groups: ', list(data_graph_labels).count(1))    \n",
    "print('fraudster_imposters: ', len(dynamic_fraudster_imposter),' number of genuine groups: ', list(data_graph_labels).count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.35  # the width of the bars\n",
    "final_honest_imposter = honest_imposter[2:]\n",
    "final_fraud_imposter = fraud_imposter[2:]\n",
    "plot_labels = [str(i+1) for i in range(2,2+len(final_fraud_imposter))]\n",
    "print(len(final_fraud_imposter),' ', len(final_honest_imposter), ' ', len(plot_labels))\n",
    "x = np.arange(len(plot_labels))  # the label locations\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, final_honest_imposter, width, label='only one geniune user')\n",
    "rects2 = ax.bar(x + width/2, final_fraud_imposter, width, label='only one fraudster user')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Number of groups')\n",
    "ax.set_xlabel('Group size')\n",
    "ax.set_title('Distribution of group fraudster with only one geniune or fraudster user Vs the group size')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(plot_labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# autolabel(rects1)\n",
    "# autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"figures_prediction/imposter_analysis_\"+name+\".pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data extraction\n",
    "\n",
    "max_nodes = 15\n",
    "test_data = []\n",
    "num_of_groups = []\n",
    "test_main_indices = []\n",
    "for i in range(num_of_intervals-1):\n",
    "    print(i)\n",
    "    test_adj_mat = np.zeros((21655,max_nodes))\n",
    "    read_path = dynamic_graphs_path + str(i+1) + '/'\n",
    "    data_ids_file = open(read_path + name +'_user_id_map.txt','r')\n",
    "    data_ids_dyn = json.load(data_ids_file)\n",
    "    data_ids_inverse_dyn = dict([(value, key) for key, value in data_ids_dyn.items()]) \n",
    "    data_adj_dyn = np.loadtxt(read_path+'_test_YELP_A.txt', delimiter=',').astype(int)\n",
    "    data_graph_indicator_dyn = np.loadtxt(read_path+name+'_graph_indicator.txt', delimiter=',').astype(int)  \n",
    "    if len(data_adj_dyn) != 0:\n",
    "        num_of_groups.append(len(np.unique(data_adj_dyn)))\n",
    "    else:\n",
    "        num_of_groups.append(0)\n",
    "    if len(data_adj_dyn) != 0:\n",
    "        data_tuple_dyn = list(map(tuple, data_adj_dyn))\n",
    "        for tuple_item in data_tuple_dyn:\n",
    "            main_idx = tuple_item[1]\n",
    "            neighbor_idx = tuple_item[0]\n",
    "            group_id = data_graph_indicator_dyn[main_idx-1]\n",
    "#             if group_id in graphs_train_idx:\n",
    "#             if group_id >= graphs_train_len:\n",
    "#                 print(group_id, graphs_train_idx.index(group_id))\n",
    "            user_id_main = data_ids_inverse_dyn[main_idx]\n",
    "            user_id_neighbor = data_ids_inverse_dyn[neighbor_idx]\n",
    "#                 group_idx = graphs_train_idx.index(group_id)\n",
    "            user_group = graphs[group_id - 1]\n",
    "            for key,value in graphs_test_dic.items():\n",
    "                if key == user_id_main:                        \n",
    "                    main_user_idx = value\n",
    "                    break\n",
    "\n",
    "            user_ids = nx.get_node_attributes(user_group,'user_id')                \n",
    "            counter = 0\n",
    "            for key,value in user_ids.items():\n",
    "                if user_ids[key] == user_id_neighbor and counter<max_nodes:                        \n",
    "                    test_adj_mat[main_user_idx-1][counter] = 1\n",
    "                    test_main_indices.append(main_user_idx-1)\n",
    "                    break\n",
    "                counter += 1\n",
    "        test_data.append(test_adj_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "total_epoch = 10\n",
    "current_epoch = 0\n",
    "time_steps = len(train_data)-1\n",
    "batch_size = 16\n",
    "hidden_size = max_nodes\n",
    "# train the lstm\n",
    "train_data_transposed = []\n",
    "for adj_mat in train_data:\n",
    "    train_data_transposed.append(np.transpose(adj_mat))\n",
    "train_data_seg = train_data[0:-1]\n",
    "target_seg = train_data[-1]\n",
    "train_batch_number = int(graphs_train_len/batch_size)\n",
    "train_data_np = np.array(train_data_seg)\n",
    "target_np = np.array(target_seg)\n",
    "many_one_model = nn.LSTM(max_nodes,hidden_size)\n",
    "while current_epoch < total_epoch:\n",
    "    for i in range(train_batch_number):        \n",
    "        input_data = train_data_np[:,i*train_batch_number:i*train_batch_number+batch_size,:]\n",
    "        target = target_np[i*train_batch_number:i*train_batch_number+batch_size,:]\n",
    "        input_data = Variable(torch.from_numpy(input_data).float())\n",
    "        target = Variable(torch.from_numpy(target).float())\n",
    "        predicted_output,_ = many_one_model(input_data)\n",
    "        last_output = predicted_output[-1,:,:]\n",
    "        print(last_output, ' ',target)    \n",
    "        error = nn.functional.binary_cross_entropy_with_logits(last_output,target)\n",
    "        error.backward()\n",
    "    current_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'gcn/data/'\n",
    "name = 'Yelp_test'\n",
    "test_main_indices = list(np.unique(np.array(test_main_indices)))\n",
    "# print(test_main_indices)\n",
    "test_data_np = np.array(test_data)\n",
    "test_scores = many_one_model(torch.from_numpy(test_data_np).float())\n",
    "test_scores_np = np.array(test_scores)\n",
    "final_test_score = test_scores_np[0].detach().numpy()\n",
    "last_snapshot = final_test_score[-1]\n",
    "\n",
    "features = np.loadtxt(path + '/YELP_node_attributes.txt',delimiter=',')\n",
    "labels = np.loadtxt(path+ '/YELP_node_labels.txt', delimiter=',').astype(int)\n",
    "\n",
    "# print(features[0])\n",
    "\n",
    "# features_list = list(feature)\n",
    "# labels_list = list(labels)\n",
    "graph_dic = {}\n",
    "label_list = []\n",
    "feature_list = []\n",
    "for index, item in enumerate(test_main_indices):\n",
    "    temp = np.zeros(len(last_snapshot[index]))\n",
    "    temp[np.argmax(np.abs(last_snapshot[index]))] = 1\n",
    "    adj_temp = []\n",
    "    adj_temp.append(index + np.argmax(np.abs(last_snapshot[index])))\n",
    "    graph_dic[index] = adj_temp \n",
    "    label2d = [0 for _ in range(2)]\n",
    "    label2d[labels[index]] = 1\n",
    "    label_list.append(label2d)\n",
    "    feature_list.append(features[index])\n",
    "\n",
    "pkl.dump(np.array(feature_list),open(save_path + 'ind.{}.tx'.format(name),'wb'))\n",
    "pkl.dump(np.array(label_list),open(save_path +'ind.{}.ty'.format(name),'wb'))    \n",
    "index_file = open(save_path+'ind.{}.test.index'.format(name),'w')\n",
    "for item in test_main_indices:    \n",
    "    index_file.write(str(item) + '\\n')\n",
    "index_file.close()    \n",
    "\n",
    "# print(last_snapshot[0], last_snapshot[1])\n",
    "# snap = [i for i in range(len(num_of_groups)) if num_of_groups[i] != 0]\n",
    "# print(len(snap))\n",
    "\n",
    "# print(len(test_scores_np),' ',len(test_scores_np[0]),' ',len(test_scores_np[0][0]),' ',len(test_scores_np[0][0][0]),' ',) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "path = 'gcn/data/'\n",
    "dataset_str = 'Amazon'\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'Yelp', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed', 'yelp', 'amazone'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# # Load data\n",
    "# adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\"\"\"\n",
    "Loads input data from gcn/data directory\n",
    "\n",
    "ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "    (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "    object;\n",
    "ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "All objects above must be saved using python pickle module.\n",
    "\n",
    ":param dataset_str: Dataset name\n",
    ":return: All data input files loaded (as well the training/test data).\n",
    "\"\"\"\n",
    "# train.py\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy, model.precision, model.recall], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "# utils.py\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "    \n",
    "#utils\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "#utils\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "#utils\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(0))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    # r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features_ch = features.dot(r_mat_inv)\n",
    "    return sparse_to_tuple(features_ch)\n",
    "\n",
    "#utils\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []\n",
    "for i in range(len(names)):\n",
    "    with open(path+\"ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "        if sys.version_info > (3, 0):\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "        else:\n",
    "            objects.append(pkl.load(f))\n",
    "            \n",
    "x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "test_idx_reorder = parse_index_file(path+\"ind.{}.test.index\".format(dataset_str))\n",
    "test_idx_range = np.sort(test_idx_reorder)\n",
    "features = sp.vstack((allx, tx)).tolil()\n",
    "features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "labels = np.vstack((ally, ty))\n",
    "labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "idx_test = test_idx_range.tolist()\n",
    "idx_train = range(len(y))\n",
    "idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "y_train = np.zeros(labels.shape)\n",
    "y_val = np.zeros(labels.shape)\n",
    "y_test = np.zeros(labels.shape)\n",
    "y_train[train_mask, :] = labels[train_mask, :]\n",
    "y_val[val_mask, :] = labels[val_mask, :]\n",
    "y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "\n",
    "# print(features[0:10])\n",
    "# Some preprocessing\n",
    "features_ch = preprocess_features(features)\n",
    "# print(features_ch[2])\n",
    "\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features_ch[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features_ch[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features_ch, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, pre, recall, duration = evaluate(features_ch, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"val_pre=\", \"{:.5f}\".format(pre), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_pre, test_recall, test_duration = evaluate(features_ch, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"precision=\", \"{:.5f}\".format(test_pre),\"recall=\",\"{:.5f}\".format(test_recall),\"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
